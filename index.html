<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Portfolio</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>

  <header>
    <div class="header-container">
        <div class="profile">
            <img src="https://i.ibb.co/CKS4NdZ/IMG-1325.png" alt="Your Avatar" class="avatar">
            <h1>Site Xiang</h1>
            <p>Data Analyst | Passionate about data visualization</p>
            <p>                                                  </p>
            <p>University of Nevada, Las Vegas Bachelor's Degree Graduate</p>
            <p>University of California, Irvine Data Analytics Certificate Holder</p>
            <p>                                                  </p>
            <p>Contact Number: 702-630-9931</p>
            <p>Email: site520mmmm@gmail.com</p>
            <p>LinkedIn: <a href="https://www.linkedin.com/in/site-xiang-09296b2bb/" target="_blank">Site Xiang on LinkedIn</a></p>
            <p>GitHub: <a href="https://github.com/SiteXiang?tab=repositories" target="_blank">SiteXiang on GitHub</a></p>

        </div>
        <div class="about-me">
            <h1>About Me</h1>
            <p>From Capturing Moments to Deciphering Data: My Professional Journey<p></p>
            Blending the art of visual storytelling with the precision of data analysis, my journey from capturing evocative moments at Caesars Palace to deciphering complex data landscapes signifies a unique fusion of creativity and analytical rigor. </p></p>With a background in commercial photography, I possess a distinctive ability to present data not just as numbers, but as stories waiting to be told. My transition into data analysis is underpinned by a profound expertise in Python, JavaScript, SQL, and machine learning, enabling me to extract, analyze, and visualize data in ways that transcend traditional analysis.</p></p>

            My skill set extends beyond the technical, incorporating a strategic understanding of how data informs decision-making, influences strategy, and drives innovation. Whether it's employing Python to automate data processes, utilizing JavaScript libraries for interactive visualizations, or leveraging machine learning algorithms to predict market trends, my approach is always rooted in delivering clear, actionable insights.</p></p> Through my work, I aim to bridge the gap between data and decision-making, turning complex datasets into compelling narratives that resonate with a diverse audience and empower informed decisions.</p></p>
            
            In the rapidly evolving field of data analysis, my mission remains constant: to harness the transformative power of data, blending analytical depth with creative flair, and to illuminate the stories hidden within datasets. Let's navigate the intricate world of data together, uncovering insights that inspire action and drive success.<p>
        </div>
    </div>
</header>

<section class="skills">
  <div class="center-content">
      <h2>My Skills</h2>
      <p>Click below for more details</p>
  </div>
</section>

    
<section class="my-skills">
  <div class="skill-item">
      <div class="accordion-item">
          <button class="accordion-button">Excel</button>
          <div class="accordion-content">
              <!-- 子手风琴按钮：项目描述 -->
              <div class="accordion-item">
                  <button class="accordion-button">Project Description</button>
                  <div class="accordion-content"><p>Project Title: Crowdfunding Data Analysis<p></p>

                    Background:<p></p>
                    Crowdfunding platforms have emerged as popular means for individuals and organizations to fund their projects. However, the success of these projects often depends on various factors. In this project, we aim to analyze a dataset containing information on 1,000 crowdfunding projects to identify trends and factors influencing their success or failure.
                    <p></p>
                    Objective:<p></p>
                    The objective of this project is to analyze a dataset of crowdfunding projects to identify patterns and insights that can inform strategies for successful fundraising campaigns.
                    <p></p>
                    Methodology:<p></p>
                    We employed exploratory data analysis techniques to examine the dataset, including data cleaning, visualization, and statistical analysis. By leveraging Excel's functionalities such as conditional formatting, pivot tables, and charts, we gained insights into the characteristics of successful and unsuccessful crowdfunding projects.
                    <p></p>
                    Key Findings:<p></p>
                    
                    Success Rates: We observed variations in the success rates of crowdfunding projects across different categories and funding goals. Certain categories, such as technology and creative projects, exhibited higher success rates compared to others.<p></p>
                    Funding Goals: Projects with moderate funding goals tended to have higher success rates compared to those with extremely high or low goals. This suggests that setting realistic funding targets is crucial for campaign success.<p></p>
                    Backer Engagement: Successful projects typically attracted a larger number of backers, indicating the importance of engaging with the audience and promoting the project effectively.<p></p>
                    Limitations:<p></p>
                    
                    The dataset may not capture all relevant factors influencing project success, such as marketing strategies and project creators' reputation.<p></p>
                    The analysis is based on historical data and may not accurately predict the success of future crowdfunding campaigns.<p></p>

                    Future Directions:<p></p>
                    Conduct sentiment analysis on project descriptions and comments to assess the impact of language and storytelling on fundraising success.<p></p>
                    Explore the relationship between project success and factors such as campaign duration and update frequency.<p></p>
                    Implement machine learning models to predict project success based on various features extracted from the dataset.<p></p>
                    
                    Conclusion:<p></p>
                    Through comprehensive analysis of crowdfunding data, we have gained valuable insights into the factors driving project success. By leveraging these insights, organizations and individuals can optimize their crowdfunding strategies to increase the likelihood of achieving their funding goals.
                    </p></div>
              </div>
              <!-- 子手风琴按钮：项目展示 -->
              <div class="accordion-item">
                  <button class="accordion-button">Project Showcase</button>
                  <div class="accordion-content">
                    <img src="https://i.ibb.co/rF72nNr/1.png" alt="Excel Showcase">
                    <img src="https://i.ibb.co/F4p4w7G/2.png" alt="Excel Showcase">
                    <img src="https://i.ibb.co/7C2TRJg/3.png" alt="Excel Showcase">
                    <img src="https://i.ibb.co/FmG6QtQ/4.png" alt="Excel Showcase">
                    <img src="https://i.ibb.co/fnFG6VQ/5.png" alt="Excel Showcase">
                    <img src="https://i.ibb.co/crZwvPY/backers01.png" alt="Excel Showcase">
                  </div>
              </div>
              <!-- 子手风琴按钮：项目代码 -->
          </div>
      </div>
  </div>

  <div class="skill-item">
    <div class="accordion-item">
        <button class="accordion-button">VBA Scripting</button>
        <div class="accordion-content">
            <!-- 子手风琴按钮：项目描述 -->
            <div class="accordion-item">
                <button class="accordion-button">Project Description</button>
                <div class="accordion-content"><p>Project Title: Stock Market Analysis using VBA Scripting<p></p>

                  Background:<p></p>
                  In this project, we utilize VBA scripting to analyze stock market data, focusing on key metrics such as yearly change, percentage change, and total stock volume. The objective is to automate the analysis process for multiple stocks over multiple years, facilitating efficient data processing and insights generation.
                  <p></p>
                  Objective:<p></p>
                  The objective of this project is to develop a VBA script that can analyze stock market data for multiple stocks over multiple years, providing essential metrics and identifying top-performing stocks based on specific criteria.
                  <p></p>
                  Methodology:<p></p>
                  We developed a VBA script that loops through each worksheet containing stock data for a given year. The script calculates the yearly change, percentage change, and total stock volume for each stock. Additionally, it identifies the stocks with the greatest percentage increase, greatest percentage decrease, and greatest total volume.
                  <p></p>
                  Key Features:<p></p>
                  Looping through Worksheets: The VBA script is designed to iterate through each worksheet, allowing for analysis of stock data for multiple years.<p></p>
                  Calculation of Metrics: The script computes the yearly change, percentage change, and total stock volume for each stock, providing insights into their performance.<p></p>
                  Identification of Top Performers: By identifying stocks with the greatest percentage increase, greatest percentage decrease, and greatest total volume, the script highlights top performers for further analysis.<p></p>
                  
                  Conclusion:<p></p>
                  By leveraging VBA scripting, we have automated the process of analyzing stock market data, making it faster and more efficient. The script provides valuable insights into stock performance, enabling investors to make informed decisions based on key metrics.</p></div>
            </div>
            <!-- 子手风琴按钮：项目展示 -->
            <div class="accordion-item">
                <button class="accordion-button">Project Showcase</button>
                <div class="accordion-content">
                  <img src="https://i.ibb.co/dL2fLQT/2020.png" alt="VBA Showcase">
                </div>
            </div>
            <!-- 子手风琴按钮：项目代码 -->
            <div class="accordion-item">
                <button class="accordion-button">Project Code</button>
                <div class="accordion-content"><pre><code>Attribute VB_Name = "Module1"
                  Sub CalculateYearlyChange()
                  
                      Set wb = ThisWorkbook
                  
                      For Each ws In wb.Sheets
                   
                          lastRow = ws.Cells(ws.Rows.Count, "A").End(xlUp).Row
                        
                          If ws.Cells(1, "A").Value = "<ticker>" And _
                             ws.Cells(1, "B").Value = "<date>" And _
                             ws.Cells(1, "C").Value = "<open>" And _
                             ws.Cells(1, "D").Value = "<high>" And _
                             ws.Cells(1, "E").Value = "<low>" And _
                             ws.Cells(1, "F").Value = "<close>" And _
                             ws.Cells(1, "G").Value = "<vol>" Then
                          
                              ws.Cells(1, "I").Value = "Ticker"
                              ws.Cells(1, "J").Value = "Yearly Change"
                              ws.Cells(1, "K").Value = "Percent Change"
                              ws.Cells(1, "L").Value = "Total Stock Volume"
                         
                              Set stockData = CreateObject("Scripting.Dictionary")
                          
                              For i = 2 To lastRow
                               
                                  ticker = ws.Cells(i, "A").Value
                                  openPrice = ws.Cells(i, "C").Value
                                  closePrice = ws.Cells(i, "F").Value
                                  volume = CDbl(ws.Cells(i, "G").Value)
                             
                                  yearlyChange = closePrice - openPrice
                                
                                  percentChange = (yearlyChange / openPrice) * 100
                               
                                  If stockData.Exists(ticker) Then
                                   
                                      existingData = stockData(ticker)
                                      existingData(0) = existingData(0) + yearlyChange
                                      existingData(1) = existingData(1) + percentChange
                                      existingData(2) = existingData(2) + volume
                                      stockData(ticker) = existingData
                                  Else
                                     
                                      Dim newData(2) As Variant
                                      newData(0) = yearlyChange
                                      newData(1) = percentChange
                                      newData(2) = volume
                                      stockData.Add ticker, newData
                                  End If
                                  
                                  ws.Cells(i, "I").ClearContents
                                  ws.Cells(i, "J").ClearContents
                                  ws.Cells(i, "K").ClearContents
                                  ws.Cells(i, "L").ClearContents
                              Next i
                             
                              RowIndex = 2
                         
                              For Each tickerKey In stockData.Keys
                              
                                  Data = stockData(tickerKey)
                                  ws.Cells(RowIndex, "I").Value = tickerKey
                                  ws.Cells(RowIndex, "J").Value = Data(0)
                                  ws.Cells(RowIndex, "K").Value = Data(1)
                                  ws.Cells(RowIndex, "K").Value = Format(Data(1), "0.00\%")
                                  ws.Cells(RowIndex, "L").Value = Data(2)
                                  RowIndex = RowIndex + 1
                              Next tickerKey
                          End If
                      Next ws
                  End Sub
                  
                  
                  Sub AddSummaryToWorksheets()
                  
                      Set wb = ThisWorkbook
                  
                      For Each ws In wb.Sheets
                       
                          lastRow = ws.Cells(ws.Rows.Count, "I").End(xlUp).Row
                  
                          maxIncrease = 0
                          maxDecrease = 0
                          maxVolume = 0
                  
                          For i = 2 To lastRow
                              percentChange = ws.Cells(i, "K").Value
                              volume = ws.Cells(i, "L").Value
                              ticker = ws.Cells(i, "I").Value
                  
                              If percentChange > maxIncrease Then
                                  maxIncrease = percentChange
                                  maxIncreaseTicker = ticker
                              End If
                  
                              If percentChange < maxDecrease Then
                                  maxDecrease = percentChange
                                  maxDecreaseTicker = ticker
                              End If
                  
                              If volume > maxVolume Then
                                  maxVolume = volume
                                  maxVolumeTicker = ticker
                              End If
                          Next i
                  
                          ws.Cells(2, "O").Value = "Greatest % Increase"
                          ws.Cells(3, "O").Value = "Greatest % Decrease"
                          ws.Cells(4, "O").Value = "Greatest Total Volume"
                          ws.Cells(1, "P").Value = "Ticker"
                          ws.Cells(1, "Q").Value = "Value"
                          ws.Cells(2, "P").Value = maxIncreaseTicker
                          ws.Cells(2, "Q").Value = Format(maxIncrease, "0.00%")
                          ws.Cells(3, "P").Value = maxDecreaseTicker
                          ws.Cells(3, "Q").Value = Format(maxDecrease, "0.00%")
                          ws.Cells(4, "P").Value = maxVolumeTicker
                          ws.Cells(4, "Q").Value = maxVolume
                      Next ws
                  End Sub
                  
                  
                  
                  Sub ColorAllSheetsCellsBasedOnValue()
                     
                      For Each ws In ThisWorkbook.Worksheets
                  
                          lastRow = ws.Cells(ws.Rows.Count, "J").End(xlUp).Row
                  
                          For Each cell In ws.Range("J2:J" & lastRow)
                              If cell.Value > 0 Then
                                  cell.Interior.Color = RGB(0, 255, 0)
                              ElseIf cell.Value < 0 Then
                                  cell.Interior.Color = RGB(255, 0, 0)
                              End If
                          Next cell
                      Next ws
                  End Sub
                  </code></pre></div>
            </div>
        </div>
    </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Python, Data Analysis, and Data Visualization</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Overview: Investigating Anti-Cancer Treatments<p></p>

                Welcome to our exploration into potential anti-cancer treatments! In this project, we dive deep into analyzing the effectiveness of various drug regimens in treating squamous cell carcinoma (SCC), a prevalent form of skin cancer.
                <p></p>
                Project Background: Unveiling Promising Treatments<p></p>
                <p></p>
                Our journey begins at Pymaceuticals, Inc., a pioneering pharmaceutical company specializing in anti-cancer medications. Tasked with spearheading this research, we were granted access to comprehensive data from their latest animal study. This study involved 249 mice diagnosed with SCC tumors, receiving treatment with a range of drug regimens over a 45-day period.
                <p></p>
                Project Objectives: Unveiling Insights<p></p>
                
                Our mission is twofold: generate all necessary tables and figures for the technical report of the clinical study and provide a top-level summary of the study results. By meticulously analyzing the data, we aim to uncover valuable insights into the efficacy of the treatments and pave the way for potential breakthroughs in cancer therapy.
                <p></p>
                Key Tasks:
                <p></p>
                Data Preparation: Merge and clean the data, ensuring accuracy and consistency for further analysis.
                <p></p>
                Summary Statistics: Compute summary statistics, including mean, median, variance, standard deviation, and SEM of tumor volume across different drug regimens.
                <p></p>
                Visualization: Create visually engaging bar charts and pie charts to depict the distribution of treatments and the gender distribution of mice in the study.
                <p></p>
                Outlier Detection: Identify potential outliers in tumor volume and visualize their distribution using box plots, highlighting key treatment regimens.
                <p></p>
                Exploratory Analysis: Explore trends in tumor volume over time for specific treatments and investigate correlations between mouse weight and tumor volume.
                <p></p>
                Project Impact: Unlocking Insights
                <p></p>
                By leveraging data-driven insights, we aim to shed light on the most promising treatments for SCC and provide actionable recommendations for further research and development. Our findings have the potential to revolutionize cancer treatment strategies and improve patient outcomes worldwide.
                <p></p>
                Join us on this transformative journey as we unravel the mysteries of anti-cancer treatments and pave the way for a brighter, healthier future.</p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <p>1. Total Number of Rows (Mouse ID/Timepoints) for Each Drug Regimen

                  Analysis:
                  
                  The bar chart illustrates the distribution of data points (Mouse ID/Timepoints) across different drug regimens.
                  
                  "Capomulin" and "Ramicane" exhibit the highest number of observations, exceeding 200. This suggests either a larger sample size or a longer duration of observation, indicating their prominence in the study.
                  
                  Other drug regimens such as "Ketapril", "Naftisol", "Zoniferol", "Placebo", "Stelasyn", "Infubinol", "Ceftamin", and "Propriva" range between 150 to 200 data points.<p>
                <img src="https://i.ibb.co/y4xFkfY/1.png" alt="Excel Showcase">
                <p>2. Distribution of Female vs Male Mice

                  Analysis:
                  
                  The pie chart reveals a balanced distribution of male and female mice within the study.
                  
                  Male mice account for 51% of the population, with female mice comprising 49%.
                  
                  This gender parity ensures robustness in data analysis by considering potential gender-related influences on drug efficacy.<p></p>
                <img src="https://i.ibb.co/QCxZW4M/2.png" alt="Excel Showcase">
                <p>3. Final Tumor Volume Distribution by Treatment

                  Analysis:
                  
                  The box plot showcases the distribution of final tumor volumes among mice treated with "Capomulin", "Ramicane", "Infubinol", and "Ceftamin".
                  
                  "Capomulin" and "Ramicane" exhibit the smallest tumor volumes, indicating their efficacy. Additionally, the narrow spread of tumor sizes suggests consistent results.
                  
                  While "Infubinol" shares a similar median tumor volume with "Ceftamin", an outlier suggests exceptional effectiveness in at least one case.
                  
                  "Ceftamin" demonstrates a slightly higher median tumor volume compared to other regimens.<p></p>
                <img src="https://i.ibb.co/PYyGSF5/3.png" alt="Excel Showcase">
                <p>4. Ramicane Treatment of Mouse k403

                  Analysis:
                  
                  The line plot tracks the tumor volume progression in mouse k403 over 45 days of "Ramicane" treatment.
                  
                  A noticeable decline in tumor volume from 45 mm3 to below 25 mm3 signifies the efficacy of "Ramicane" in reducing tumor size for this specific mouse.<p></p>
                <img src="https://i.ibb.co/Fb6xVWZ/4.png" alt="Excel Showcase">
                <p>5. Mouse Weight vs Average Tumor Volume for Capomulin Regimen

                  Analysis:
                  
                  The scatter plot, along with the regression line, explores the relationship between mouse weight and average tumor volume under the "Capomulin" regimen.
                  
                  A positive correlation is observed, indicating that as mouse weight increases, average tumor volume tends to rise.
                  
                  The regression equation (y=0.95x+21.55) allows for predicting average tumor volume based on mouse weight.<p></p>
                <img src="https://i.ibb.co/GHQ84tW/5.png" alt="Excel Showcase">
                <p>Conclusion:

                  The analysis sheds light on the efficacy of various drug regimens in treating tumors. "Capomulin" and "Ramicane" demonstrate potential in reducing tumor sizes, while mouse weight appears to influence tumor volume within the "Capomulin" regimen.
                  
                  Further rigorous investigation and statistical validation are recommended to identify and prioritize the most promising treatments for future research endeavors.<p></p>
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code># Dependencies and Setup
                import matplotlib.pyplot as plt
                import pandas as pd
                import scipy.stats as st
                
                # Study data files
                mouse_metadata_path = "data/Mouse_metadata.csv"
                study_results_path = "data/Study_results.csv"
                
                # Read the mouse data and the study results
                mouse_metadata = pd.read_csv(mouse_metadata_path)
                study_results = pd.read_csv(study_results_path)
                
                # Combine the data into a single DataFrame
                combined_data = pd.merge(mouse_metadata, study_results, on="Mouse ID", how="outer")
                
                # Checking the number of mice.
                num_mice = combined_data['Mouse ID'].nunique()
                
                # Get the duplicate mice by ID number that shows up for Mouse ID and Timepoint.
                duplicate_mice = combined_data.loc[combined_data.duplicated(subset=['Mouse ID', 'Timepoint']), 'Mouse ID'].unique()
                
                # Create a clean DataFrame by dropping the duplicate mouse by its ID.
                clean_data = combined_data[combined_data['Mouse ID'].isin(duplicate_mice)==False]
                
                # Checking the number of mice in the clean DataFrame.
                num_mice_clean = clean_data['Mouse ID'].nunique()
                
                # Summary Statistics
                
                # Group by Drug Regimen
                drug_regimen_group = clean_data.groupby('Drug Regimen')
                
                # Calculate mean, median, variance, standard deviation, and SEM of the tumor volume
                tumor_volume_mean = drug_regimen_group['Tumor Volume (mm3)'].mean()
                tumor_volume_median = drug_regimen_group['Tumor Volume (mm3)'].median()
                tumor_volume_variance = drug_regimen_group['Tumor Volume (mm3)'].var()
                tumor_volume_std = drug_regimen_group['Tumor Volume (mm3)'].std()
                tumor_volume_sem = drug_regimen_group['Tumor Volume (mm3)'].sem()
                
                # Create summary dataframe
                summary_df = pd.DataFrame({"Mean": tumor_volume_mean,
                                           "Median": tumor_volume_median,
                                           "Variance": tumor_volume_variance,
                                           "Standard Deviation": tumor_volume_std,
                                           "SEM": tumor_volume_sem})
                
                # Bar and Pie Charts
                
                # Group by Drug Regimen
                drug_counts = clean_data['Drug Regimen'].value_counts()
                
                # Plot the counts using pandas
                drug_counts.plot(kind='bar', color='blue', alpha=0.7)
                
                plt.xlabel('Drug Regimen')
                plt.ylabel('# of Observed Mouse Timepoints')
                plt.title('Total Number of Rows (Mouse ID/Timepoints) for Each Drug Regimen')
                plt.tight_layout()
                plt.show()
                
                # Create x-axis
                x_axis = drug_counts.index.values
                
                # Plot the counts using pyplot
                plt.bar(x_axis, drug_counts, color='blue', alpha=0.7, align='center')
                
                plt.xlabel('Drug Regimen')
                plt.ylabel('# of Observed Mouse Timepoints')
                plt.title('Total Number of Rows (Mouse ID/Timepoints) for Each Drug Regimen')
                plt.xticks(rotation=90)
                plt.tight_layout()
                plt.show()
                
                # Generate a pie plot showing the distribution of female versus male mice using Pandas
                gender_distribution = clean_data['Sex'].value_counts()
                
                gender_distribution.plot(kind='pie', autopct='%1.1f%%', colors=['blue', 'orange'], ylabel='Sex')
                plt.title('Distribution of Female vs Male Mice')
                plt.show()
                
                # Generate a pie plot showing the distribution of female versus male mice using pyplot
                labels = gender_distribution.index
                sizes = gender_distribution.values
                colors = ['blue', 'orange']
                explode = (0, 0)
                
                plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%')
                plt.ylabel('Sex')
                plt.axis('equal')
                plt.title('Distribution of Female vs Male Mice')
                plt.show()
                
                # Calculate the final tumor volume of each mouse across four of the treatment regimens
                last_timepoint = clean_data.groupby("Mouse ID")["Timepoint"].max()
                
                # Merge this group df with the original DataFrame to get the tumor volume at the last timepoint
                merged_df = pd.merge(clean_data, last_timepoint, on=["Mouse ID", "Timepoint"])
                
                # Put treatments into a list for for loop
                treatments = ["Capomulin", "Ramicane", "Infubinol", "Ceftamin"]
                
                # Create empty list to fill with tumor vol data
                tumor_vol_data = []
                
                # Calculate the IQR and quantitatively determine if there are any potential outliers
                for treatment in treatments:
                    final_tumor_vol = merged_df.loc[merged_df["Drug Regimen"] == treatment, "Tumor Volume (mm3)"]
                    tumor_vol_data.append(final_tumor_vol)
                    quartiles = final_tumor_vol.quantile([.25,.5,.75])
                    lowerq = quartiles[0.25]
                    upperq = quartiles[0.75]
                    iqr = upperq-lowerq
                    lower_bound = lowerq - (1.5*iqr)
                    upper_bound = upperq + (1.5*iqr)
                    outliers = final_tumor_vol[(final_tumor_vol < lower_bound) | (final_tumor_vol > upper_bound)]
                    print(f"{treatment}'s potential outliers: {outliers}")
                
                # Generate a box plot that shows the distrubution of the tumor volume for each treatment group.
                flierprops = dict(marker='o', markerfacecolor='red', markersize=12, linestyle='none', markeredgecolor='black')
                
                fig, ax = plt.subplots()
                ax.boxplot(tumor_vol_data, flierprops=flierprops)
                ax.set_title('Final Tumor Volume Distribution by Treatment')
                ax.set_ylabel('Final Tumor Volume (mm3)')
                ax.set_xlabel('Drug Regimen')
                ax.set_xticklabels(treatments)
                plt.show()
                
                # Line and Scatter Plots
                
                mouse_id = 'k403'
                mouse_data = clean_data[clean_data['Mouse ID'] == mouse_id]
                
                plt.figure(figsize=(8, 6))
                plt.plot(mouse_data['Timepoint'], mouse_data['Tumor Volume (mm3)'], '-o', color='blue')
                plt.title(f'{mouse_data["Drug Regimen"].iloc[0]} treatment of mouse {mouse_id}')
                plt.xlabel('Timepoint (days)')
                plt.ylabel('Tumor Volume (mm3)')
                plt.grid(True, which='both', linestyle='--', linewidth=0.5)
                plt.tight_layout()
                plt.show()
                
                capomulin_data = clean_data[clean_data['Drug Regimen'] == "Capomulin"]
                
                capomulin_avg = capomulin_data.groupby('Mouse ID').agg({'Tumor Volume (mm3)': 'mean', 'Weight (g)': 'mean'})
                
                plt.figure(figsize=(8, 6))
                plt.scatter(capomulin_avg['Weight (g)'], capomulin_avg['Tumor Volume (mm3)'], color='blue')
                plt.title('Mouse Weight vs Average Tumor Volume for Capomulin Regimen')
                plt.xlabel('Weight (g)')
                plt.ylabel('Average Tumor Volume (mm3)')
                plt.grid(True, linestyle='--', linewidth=0.5)
                plt.tight_layout()
                plt.show()
                
                # Correlation and Regression
                
                x_values = capomulin_avg['Weight (g)']
                y_values = capomulin_avg['Tumor Volume (mm3)']
                
                correlation = st.pearsonr(x_values,y_values)
                print(f"The correlation between mouse weight and average tumor volume is {round(correlation[0],2)}")
                
                (slope, intercept, rvalue, pvalue, stderr) = st.linregress(x_values, y_values)
                regress_values = x_values * slope + intercept
                line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
                
                plt.figure(figsize=(8, 6))
                plt.scatter(x_values,y_values, color='blue')
                plt.plot(x_values,regress_values,"r-")
                plt.annotate(line_eq,(20,36),fontsize=15,color="red")
                plt.title('Mouse Weight vs Average Tumor Volume for Capomulin Regimen')
                plt.xlabel('Weight (g)')
                plt.ylabel('Average Tumor Volume (mm3)')
                plt.grid(True, linestyle='--', linewidth=0.5)
                plt.tight_layout()
                plt.show()
                </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Slope, Correlation, p-value, Standard Error</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Comprehensive Analysis of Public Health Data Across U.S. Cities<p></p>
                Project Overview<p></p>
                In this project, we embarked on a detailed examination of public health data sourced from the Centers for Disease Control and Prevention (CDC), focusing on the 500 Cities project. The aim was to uncover insights into the prevalence of various health conditions and preventive measures across different locales within the United States. Utilizing advanced data analytics techniques, we dissected trends in metabolic health, cardiovascular health, respiratory health, mental health, and cancer screening practices, thereby providing a holistic view of public health across the nation.
                <p></p>
                Data Source and Methodology<p></p>
                The dataset was acquired through a structured query from the CDC's public API, ensuring up-to-date and comprehensive health data across multiple cities. The initial dataset comprised extensive metrics, including but not limited to obesity, diabetes, high cholesterol, coronary heart disease, asthma, mental health issues, and screening rates for various cancers.
                <p></p>
                Our methodology entailed a meticulous data cleaning process to refine the dataset for analysis, eliminating irrelevant columns and focusing on the most impactful health indicators. Subsequent data analysis involved statistical methods and visualization techniques to explore correlations between different health conditions and screenings. For instance, we assessed the relationship between obesity, diabetes, and high cholesterol levels; analyzed the link between coronary heart disease and high blood pressure; and examined the association between respiratory health indicators such as COPD and asthma prevalence.
                <p></p>
                Key Findings and Insights<p></p>
                The analysis revealed significant correlations between various health conditions, highlighting the intertwined nature of metabolic, cardiovascular, and respiratory health. Notably, a strong correlation was observed between obesity and diabetes prevalence, underscoring the critical need for targeted public health interventions in these areas. Our findings also pointed to the importance of cancer screening practices, with data suggesting disparities in screening rates across different regions.
                <p></p>
                In addition to individual health conditions, our analysis provided insights into the broader societal impact of poor health, including the relationship between mental and physical health. The statistical significance of these correlations was validated through rigorous testing, including slope analysis, test statistics, p-values, and standard error measurements, ensuring the robustness of our conclusions.
                <p></p>
                Implications and Future Directions<p></p>
                This project not only sheds light on the current state of public health in the U.S. but also serves as a foundation for future research and policy-making. By identifying areas with high prevalence rates of various conditions, policymakers and healthcare providers can allocate resources more effectively, design targeted interventions, and ultimately improve the overall health and well-being of communities.
                <p></p>
                Furthermore, the methodology applied in this project can be adapted to other datasets and health indicators, offering a scalable approach for ongoing public health research. Our findings underscore the potential of data-driven analysis in shaping public health strategies and enhancing the quality of life for individuals across the nation.
                <p></p>
                Conclusion<p></p>
                Through a comprehensive analysis of CDC public health data, this project provides critical insights into the prevalence and correlations of major health conditions and screenings across U.S. cities. By leveraging advanced analytics and visualization techniques, we have illuminated the complex interplay of various health factors, offering valuable perspectives for healthcare professionals, researchers, and policymakers alike.<p></p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <p>Relationship between Obesity and Diabetes: There exists a significant positive correlation between obesity and diabetes. With a slope of 0.5555, for every one-percentage-point increase in obesity, diabetes is expected to increase by approximately 0.5555 percentage points. The correlation coefficient of 0.8451 indicates a strong relationship.

                  Relationship between High Cholesterol and Diabetes: There is also a significant positive correlation between high cholesterol and diabetes. With a slope of 0.6337, for every one-percentage-point increase in high cholesterol, diabetes is expected to increase by approximately 0.6337 percentage points. The correlation coefficient of 0.7027 suggests a moderate strength of relationship.
                  
                  Relationship between High Cholesterol and Obesity: Lastly, there is a significant but weaker positive correlation between high cholesterol and obesity. With a slope of 0.4774, for every one-percentage-point increase in high cholesterol, obesity is expected to increase by approximately 0.4774 percentage points. The correlation coefficient of 0.3480 indicates a relatively weaker relationship.
                  
                  In summary, all three pairs of variables demonstrate a trend where higher levels of high cholesterol and obesity correspond to increased diabetes rates. However, the relationship between obesity and diabetes is the most significant and strongest, followed by the relationship between high cholesterol and diabetes, with the relationship between high cholesterol and obesity being the weakest among the three.
                  <p></p>
                <img src="https://i.ibb.co/yN5h6mx/1.png" alt="Excel Showcase">
                <p>Relationship between High Blood Pressure and Coronary Heart Disease: The analysis reveals a significant positive correlation between high blood pressure and coronary heart disease. The slope of 3.7354 indicates that for every one-percentage-point increase in coronary heart disease prevalence, high blood pressure prevalence is expected to increase by approximately 3.7354 percentage points. The correlation coefficient of 0.8927 indicates a strong relationship.

                  Relationship between Cholesterol Screening and Coronary Heart Disease: There is also a significant positive correlation between cholesterol screening and coronary heart disease. The slope of 0.7321 suggests that for every one-percentage-point increase in coronary heart disease prevalence, cholesterol screening prevalence increases by approximately 0.7321 percentage points. However, the correlation coefficient of 0.2881 indicates a weaker relationship compared to high blood pressure and coronary heart disease.
                  
                  Relationship between Cholesterol Screening and High Blood Pressure: Lastly, there is a significant but weaker positive correlation between cholesterol screening and high blood pressure. The slope of 0.1915 suggests that for every one-percentage-point increase in high blood pressure prevalence, cholesterol screening prevalence increases by approximately 0.1915 percentage points. The correlation coefficient of 0.3154 indicates a relatively weaker relationship compared to the other analyses.
                  
                  In conclusion, the analysis demonstrates correlations between high blood pressure, coronary heart disease, and cholesterol screening, albeit with varying strengths and directions. The relationship between high blood pressure and coronary heart disease appears the most significant and strongest, followed by the relationship between cholesterol screening and coronary heart disease, while the relationship between cholesterol screening and high blood pressure is comparatively weaker.<p></p>
                <img src="https://i.ibb.co/RgN06gD/2.png" alt="Excel Showcase">
                <p>The analysis investigated the relationship between asthma and COPD (Chronic Obstructive Pulmonary Disease) prevalence. The results indicate:

                  Positive Correlation: There is a strong positive correlation (r = 0.7443) between asthma and COPD prevalence. This suggests that as the prevalence of COPD increases, the prevalence of asthma also tends to increase.
                  
                  Significance: The p-value of 3.559069907244164e-177 indicates that the observed correlation is statistically significant. This means there is strong evidence against the null hypothesis, supporting the presence of a significant correlation between asthma and COPD.
                  
                  Slope: The slope of the regression line is 0.3939, suggesting that for every one-percentage-point increase in COPD prevalence, asthma prevalence is expected to increase by approximately 0.3939 percentage points.
                  
                  Precision: The standard error of the slope is 0.0112, indicating a relatively precise estimate of the slope.
                  
                  In conclusion, the analysis highlights a substantial and statistically significant positive correlation between asthma and COPD prevalence. This finding underscores the importance of understanding and managing these respiratory conditions, as they often coexist and may share common risk factors and treatment approaches.<p></p>
                <img src="https://i.ibb.co/D8JbfTK/3.png" alt="Excel Showcase">
                <p>Slope: The slope of the regression line is 0.9256. It indicates that for every one-percentage-point increase in poor mental health prevalence, poor physical health prevalence is expected to increase by approximately 0.9256 percentage points.

                  Correlation Coefficient (Test Statistic): The correlation coefficient (r) is 0.8114, suggesting a strong positive correlation between poor mental health and poor physical health prevalence.
                  
                  p-value: The p-value is extremely low (5.184613953988032e-235), indicating strong evidence against the null hypothesis (no correlation). This suggests that the observed correlation between poor mental health and poor physical health is statistically significant.
                  
                  Standard Error of the Slope: The standard error of the slope is 0.0211. It indicates the precision of the estimated slope. A lower value suggests a more precise estimate.
                  
                  In summary, the analysis demonstrates a significant positive correlation between poor mental health and poor physical health. As the prevalence of poor mental health increases, the prevalence of poor physical health also tends to increase, highlighting the interconnectedness between mental and physical well-being.<p></p>
                <img src="https://i.ibb.co/xFcTrX2/4.png" alt="Excel Showcase">
                <p>Slope: The slope of the regression line is 0.5416. It indicates that for every one-percentage-point increase in mammography use prevalence, pap test prevalence is expected to increase by approximately 0.5416 percentage points.

                  Correlation Coefficient (Test Statistic): The correlation coefficient (r) is 0.8415, suggesting a strong positive correlation between mammography use and pap test prevalence.
                  
                  p-value: The p-value is extremely low (3.7296370647172155e-107), indicating strong evidence against the null hypothesis (no correlation). This suggests that the observed correlation between mammography use and pap test prevalence is statistically significant.
                  
                  Standard Error of the Slope: The standard error of the slope is 0.0175. It indicates the precision of the estimated slope. A lower value suggests a more precise estimate.
                  
                  In summary, the analysis demonstrates a significant positive correlation between mammography use and pap test prevalence. As the prevalence of mammography use increases, the prevalence of pap test also tends to increase, indicating a potential association between these two screening practices for women's health.
                  <p></p>
                <img src="https://i.ibb.co/xFcTrX2/4.png" alt="Excel Showcase">
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code># Import the requests library.
                import pandas as pd
                import requests 
                import json 
                
                # Set the request parameters.
                app_token = 'app_token'  
                api_endpoint = 'https://data.cdc.gov/resource/k86t-wghb.json'  
                headers = {
                    'X-App-Token': app_token,  
                    'Content-Type': 'application/json'}
                
                # Make the HTTP request.
                response = requests.get(api_endpoint, headers=headers)  
                response.json()  
                
                # Print the JSON response from the API call.
                print(json.dumps(response.json(), indent=4, sort_keys=True))
                
                # Make a dataframe from the json response
                df = pd.DataFrame(response.json())
                df.head()
                
                # Drop all columns that with 95ci in the name, and drop placefips, tractfips, and place_tractid
                df = df.drop(df.filter(regex='95ci').columns, axis=1)
                df = df.drop(['placefips','tractfips','place_tractid'], axis=1)
                df.head()
                
                # Group 1: Metabolic Health
                # Obesity (obesity_crudeprev)
                # Diabetes (diabetes_crudeprev)
                # High Cholesterol (highchol_crudeprev)
                
                import matplotlib.pyplot as plt
                import seaborn as sns
                import numpy as np
                
                # Convert column data types to float
                df['obesity_crudeprev'] = df['obesity_crudeprev'].astype(float)
                df['diabetes_crudeprev'] = df['diabetes_crudeprev'].astype(float)
                df['highchol_crudeprev'] = df['highchol_crudeprev'].astype(float)
                
                # Create scatter plot, set point size to the value of highchol_crudeprev column, color gradient represents the value of highchol_crudeprev
                plt.figure(figsize=(10, 10))
                sns.scatterplot(x='obesity_crudeprev', y='diabetes_crudeprev', size='highchol_crudeprev', hue='highchol_crudeprev', palette='viridis', data=df, alpha=0.7)
                
                # Add regression line
                sns.regplot(x='obesity_crudeprev', y='diabetes_crudeprev', data=df, scatter=False, color='red')
                
                # Calculate regression equation parameters
                slope, intercept = np.polyfit(df['obesity_crudeprev'], df['diabetes_crudeprev'], 1)
                
                # Add regression equation to the plot
                plt.annotate(f'y = {slope:.2f}x + {intercept:.2f}', (45,3), fontsize=12, color='red')
                
                # Add title and labels
                plt.title('Scatter Plot of Diabetes vs. Obesity with High Cholesterol and Regression Line')
                plt.xlabel('Obesity Crude Prevalence')
                plt.ylabel('Diabetes Crude Prevalence')
                
                # Show legend
                plt.legend(title='High Cholesterol Crude Prev')
                
                # Display the plot
                plt.show()
                
                # Get the highest obesity_crudeprev, diabetes_crudeprev, and highchol_crudeprev placename
                print('the highest obesity_crudeprev placename is: ', df.loc[df['obesity_crudeprev'].idxmax()]['placename'])
                print('the highest diabetes_crudeprev placename is: ', df.loc[df['diabetes_crudeprev'].idxmax()]['placename'])
                print('the highest highchol_crudeprev placename is: ', df.loc[df['highchol_crudeprev'].idxmax()]['placename'])
                
                # Get the top 5 highest obesity_crudeprev, diabetes_crudeprev, and highchol_crudeprev placename and values
                print('the top 5 highest obesity_crudeprev placename and values are: ')
                print(df.nlargest(5, 'obesity_crudeprev')[['placename', 'obesity_crudeprev']])
                print('the top 5 highest diabetes_crudeprev placename and values are: ')
                print(df.nlargest(5, 'diabetes_crudeprev')[['placename', 'diabetes_crudeprev']])
                print('the top 5 highest highchol_crudeprev placename and values are: ')
                print(df.nlargest(5, 'highchol_crudeprev')[['placename', 'highchol_crudeprev']])
                
                # Get the top 5 highest obesity_crudeprev, diabetes_crudeprev, and highchol_crudeprev unique state and their values
                print('the top 5 highest diabetes_crudeprev unique state and their values are: ')
                print(df.groupby('stateabbr').diabetes_crudeprev.max().nlargest(5))
                print('the top 5 highest highchol_crudeprev unique state and their values are: ')
                print(df.groupby('stateabbr').highchol_crudeprev.max().nlargest(5))
                
                # Get the slope, test statistic, p-value, and standard error of the slope, correlation coefficient between obesity and diabetes
                from scipy import stats
                slope, intercept, r_value, p_value, std_err = stats.linregress(df['obesity_crudeprev'], df['diabetes_crudeprev'])
                print(f'Slope: {slope}')
                print(f'Test Statistic: {r_value}')
                print(f'p-value: {p_value}')
                print(f'Standard Error of the Slope: {std_err}')
                print(f'Correlation Coefficient: {r_value}')
                
                # Relationship between high cholesterol and diabetes
                slope, intercept, r_value, p_value, std_err = stats.linregress(df['highchol_crudeprev'], df['diabetes_crudeprev'])
                print(f'Slope: {slope}')
                print(f'Test Statistic: {r_value}')
                print(f'p-value: {p_value}')
                print(f'Standard Error of the Slope: {std_err}')
                print(f'Correlation Coefficient: {r_value}')
                
                # Relationship between high cholesterol and obesity
                slope, intercept, r_value, p_value, std_err = stats.linregress(df['highchol_crudeprev'], df['obesity_crudeprev'])
                print(f'Slope: {slope}')
                print(f'Test Statistic: {r_value}')
                print(f'p-value: {p_value}')
                print(f'Standard Error of the Slope: {std_err}')
                print(f'Correlation Coefficient: {r_value}')
                
                # For cardiovascular health: Coronary Heart Disease (CHD), High Blood Pressure (BPHigh), and Cholesterol Screening (CholScreen)
                df['chd_crudeprev'] = df['chd_crudeprev'].astype(float)
                df['bphigh_crudeprev'] = df['bphigh_crudeprev'].astype(float)
                df['cholscreen_crudeprev'] = df['cholscreen_crudeprev'].astype(float)
                
                # Scatter plot for CHD vs. BPHigh with Cholesterol Screening
                plt.figure(figsize=(10, 10))
                sns.scatterplot(x='chd_crudeprev', y='bphigh_crudeprev', size='cholscreen_crudeprev', hue='cholscreen_crudeprev', palette='viridis', data=df, alpha=0.7)
                sns.regplot(x='chd_crudeprev', y='bphigh_crudeprev', data=df, scatter=False, color='red')
                slope, intercept = np.polyfit(df['chd_crudeprev'], df['bphigh_crudeprev'], 1)
                plt.annotate(f'y = {slope:.2f}x + {intercept:.2f}', (15,10), fontsize=12, color='red')
                plt.title('Scatter Plot of High Blood Pressure vs. Coronary Heart Disease with Cholesterol Screening and Regression Line')
                plt.xlabel('Coronary Heart Disease Crude Prevalence')
                plt.ylabel('High Blood Pressure Crude Prevalence')
                plt.legend(title='Cholesterol Screening Crude Prev')
                plt.show()
                
                # Statistical analysis for relationships among CHD, BPHigh, and Cholesterol Screening
                slope, intercept, r_value, p_value, std_err = stats.linregress(df['chd_crudeprev'], df['bphigh_crudeprev'])
                print(f'Slope: {slope}')
                print(f'Test Statistic: {r_value}')
                print(f'p-value: {p_value}')
                print(f'Standard Error of the Slope: {std_err}')
                print(f'Correlation Coefficient: {r_value}')
                
                slope, intercept, r_value, p_value, std_err = stats.linregress(df['chd_crudeprev'], df['cholscreen_crudeprev'])
                print(f'Slope: {slope}')
                print(f'Test Statistic: {r_value}')
                print(f'p-value: {p_value}')
                print(f'Standard Error of the Slope: {std_err}')
                print(f'Correlation Coefficient: {r_value}')
                
                slope, intercept, r_value, p_value, std_err = stats.linregress(df['bphigh_crudeprev'], df['cholscreen_crudeprev'])
                print(f'Slope: {slope}')
                print(f'Test Statistic: {r_value}')
                print(f'p-value: {p_value}')
                print(f'Standard Error of the Slope: {std_err}')
                print(f'Correlation Coefficient: {r_value}')
                print('-'*50)
                
                # For respiratory health: COPD and Childhood Asthma
                df['copd_crudeprev'] = df['copd_crudeprev'].astype(float)
                df['casthma_crudeprev'] = df['casthma_crudeprev'].astype(float)
                
                # Scatter plot for COPD vs. Childhood Asthma
                plt.figure(figsize=(10, 10))
                sns.scatterplot(x='copd_crudeprev', y='casthma_crudeprev', palette='viridis', data=df, alpha=0.7)
                sns.regplot(x='copd_crudeprev', y='casthma_crudeprev', data=df, scatter=False, color='red')
                slope, intercept = np.polyfit(df['copd_crudeprev'], df['casthma_crudeprev'], 1)
                plt.annotate(f'y = {slope:.2f}x + {intercept:.2f}', (12,8), fontsize=12, color='red')
                plt.title('Scatter Plot of Asthma vs. COPD with Regression Line')
                plt.xlabel('COPD Crude Prevalence')
                plt.ylabel('Asthma Crude Prevalence')
                plt.legend(title='copd_crudeprev')
                plt.show()
                
                # Statistical analysis for relationship between Asthma and COPD
                slope, intercept, r_value, p_value, std_err = stats.linregress(df['copd_crudeprev'], df['casthma_crudeprev'])
                print(f'Slope: {slope}')
                print(f'Test Statistic: {r_value}')
                print(f'p-value: {p_value}')
                print(f'Standard Error of the Slope: {std_err}')
                print(f'Correlation Coefficient: {r_value}')
                print('-'*50)
                
                # For mental health: Poor Mental Health and Poor Physical Health
                df['mhlth_crudeprev'] = df['mhlth_crudeprev'].astype(float)
                df['phlth_crudeprev'] = df['phlth_crudeprev'].astype(float)
                
                # Scatter plot for Poor Mental Health vs. Poor Physical Health
                plt.figure(figsize=(10, 10))
                sns.scatterplot(x='mhlth_crudeprev', y='phlth_crudeprev', palette='viridis', data=df, alpha=0.7)
                sns.regplot(x='mhlth_crudeprev', y='phlth_crudeprev', data=df, scatter=False, color='red')
                slope, intercept = np.polyfit(df['mhlth_crudeprev'], df['phlth_crudeprev'], 1)
                plt.annotate(f'y = {slope:.2f}x + {intercept:.2f}', (22,8), fontsize=12, color='red')
                plt.title('Scatter Plot of Poor Mental Health vs. Poor Physical Health with Regression Line')
                plt.xlabel('Poor Mental Health Crude Prevalence')
                plt.ylabel('Poor Physical Health Crude Prevalence')
                plt.legend(title='mhlth_crudeprev')
                plt.show()
                
                # Statistical analysis for relationship between Poor Mental Health and Poor Physical Health
                slope, intercept, r_value, p_value, std_err = stats.linregress(df['mhlth_crudeprev'], df['phlth_crudeprev'])
                print(f'Slope: {slope}')
                print(f'Test Statistic: {r_value}')
                print(f'p-value: {p_value}')
                print(f'Standard Error of the Slope: {std_err}')
                print(f'Correlation Coefficient: {r_value}')
                print('-'*50)</code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Python APIs</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project: Global Weather Analysis and Vacation Planning<p></p>

                Project Overview:<p></p>
                Global Weather Analysis and Vacation Planning is a Python-based data analysis project aimed at exploring the relationship between city weather and distance from the equator by leveraging the OpenWeatherMap API. Through data visualization and statistical analysis, the project provides insights into weather patterns across different cities globally and offers data-driven support for vacation planning.
                <p></p>
                Key Features:<p></p>
                
                Utilizes the OpenWeatherMap API to retrieve weather data for cities worldwide, covering key metrics such as temperature, humidity, cloudiness, and wind speed.<p></p>
                Generates scatter plots to visualize the relationship between latitude and various weather indicators, aiding users in understanding weather characteristics across different regions.<p></p>
                Performs linear regression analysis to investigate the relationship between city latitude and temperature, humidity, cloudiness, and wind speed, providing users with scientific insights.<p></p>
                Integrates Geoapify API and map visualization techniques to offer users a convenient tool for vacation planning, assisting them in finding ideal holiday destinations.<p></p>
                Provides a user-friendly interface and interactive experience, enabling users to effortlessly conduct city weather analysis and vacation planning tasks.<p></p>
                Applications:<p></p>
                Global Weather Analysis and Vacation Planning project can be widely applied in the following scenarios:<p></p>
                
                Travel Planning: Users can utilize the tool to explore weather conditions in cities worldwide and choose ideal vacation destinations.<p></p>
                Academic Research: Researchers can use the tool to perform statistical analysis on city weather data and investigate the relationship between weather and geographic location.<p></p>
                Travel Industry: Travel professionals can leverage the tool to provide personalized travel recommendations and planning services to clients, enhancing user satisfaction and travel experiences.<p></p>
                Project Deliverables:<p></p>
                Through the Global Weather Analysis and Vacation Planning project, users gain intuitive insights into global city weather conditions and receive data-driven support for vacation planning, enabling them to enjoy more pleasant and seamless travel experiences.
                <p></p>
                The project showcases the application of data analysis and visualization techniques in weather data processing and travel planning domains, offering practical value and potential for adoption across various fields and user groups.
                <p></p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <img src="https://i.ibb.co/rwr8HCj/v1.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/G0dMWP4/v2.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/bQ8VZBp/w1.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/6P9WCvB/w2.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/fd7H658/w3.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/m9Jzgq5/w4.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/pft5N5J/w5.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/9WZxCGB/w6.png" alt="Excel Showcase">
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code># Dependencies and Setup
                import matplotlib.pyplot as plt
                import pandas as pd
                import numpy as np
                import requests
                import time
                from scipy.stats import linregress
                
                # Import the OpenWeatherMap API key
                from api_keys import weather_api_key
                
                # Import citipy to determine the cities based on latitude and longitude
                from citipy import citipy
                
                # Generate the Cities List by Using the citipy Library
                # Define the range of latitudes and longitudes
                lat_range = (-90, 90)
                lng_range = (-180, 180)
                
                # Create empty lists for holding the latitude and longitude combinations and city names
                lat_lngs = []
                cities = []
                
                # Create a set of random latitudes and longitudes
                lats = np.random.uniform(lat_range[0], lat_range[1], size=1500)
                lngs = np.random.uniform(lng_range[0], lng_range[1], size=1500)
                lat_lngs = zip(lats, lngs)
                
                # Identify nearest city for each lat, lng combination
                for lat_lng in lat_lngs:
                    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
                    
                    # Add the city to the list if it's unique
                    if city not in cities:
                        cities.append(city)
                
                # Print the city count to confirm sufficient count
                print(f"Number of cities in the list: {len(cities)}")
                
                # Use the OpenWeatherMap API to retrieve weather data from the cities list generated
                # Set the API base URL
                url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + weather_api_key
                
                # Define an empty list to fetch the weather data for each city
                city_data = []
                
                # Print to logger
                print("Beginning Data Retrieval     ")
                print("-----------------------------")
                
                # Create counters
                record_count = 1
                set_count = 1
                
                # Loop through all the cities in our list to fetch weather data
                for i, city in enumerate(cities):
                        
                    # Group cities in sets of 50 for logging purposes
                    if (i % 50 == 0 and i >= 50):
                        set_count += 1
                        record_count = 0
                
                    # Create endpoint URL with each city
                    city_url = f"{url}&q={city}"
                    
                    # Log the url, record, and set numbers
                    print("Processing Record %s of Set %s | %s" % (record_count, set_count, city))
                
                    # Add 1 to the record count
                    record_count += 1
                
                    # Run an API request for each of the cities
                    try:
                        # Parse the JSON and retrieve data
                        city_weather = requests.get(city_url).json()
                
                        # Parse out latitude, longitude, max temp, humidity, cloudiness, wind speed, country, and date
                        city_lat = city_weather["coord"]["lat"]
                        city_lng = city_weather["coord"]["lon"]
                        city_max_temp = city_weather['main']['temp_max']
                        city_humidity = city_weather['main']['humidity']
                        city_clouds = city_weather['clouds']['all']
                        city_wind = city_weather['wind']['speed']
                        city_country = city_weather['sys']['country']
                        city_date = city_weather['dt']
                
                        # Append the City information into city_data list
                        city_data.append({"City": city, 
                                          "Lat": city_lat, 
                                          "Lng": city_lng, 
                                          "Max Temp": city_max_temp,
                                          "Humidity": city_humidity,
                                          "Cloudiness": city_clouds,
                                          "Wind Speed": city_wind,
                                          "Country": city_country,
                                          "Date": city_date})
                
                    # If an error is experienced, skip the city
                    except:
                        print("City not found. Skipping...")
                        pass
                              
                # Indicate that Data Loading is complete 
                print("-----------------------------")
                print("Data Retrieval Complete      ")
                print("-----------------------------")
                
                # Convert the cities weather data into a Pandas DataFrame
                city_data_df = pd.DataFrame(city_data)
                
                # Show Record Count
                city_data_df.count()
                
                # Display sample data
                city_data_df.head()
                
                # Export the City_Data into a csv
                city_data_df.to_csv("output_data/cities.csv", index_label="City_ID")
                
                # Read saved data
                city_data_df = pd.read_csv("output_data/cities.csv", index_col="City_ID")
                
                # Display sample data
                city_data_df.head()
                
                # Create the Scatter Plots Resquested
                
                # Latitude Vs. Temperture
                # Create scatter plot
                plt.figure(figsize=(10, 10))
                plt.scatter(city_data_df['Lat'], city_data_df['Max Temp'], color='blue', edgecolor='black', alpha=0.75)
                
                # Add title and labels
                plt.title(f"City Latitude vs. Max Temperature ({time.strftime('%Y-%m-%d')})")
                plt.xlabel('Latitude')
                plt.ylabel('Max Temperature (°F)')
                plt.grid(True)
                
                # Show plot
                plt.show()
                
                # Latitude Vs. Humidity
                # Create scatter plot
                plt.figure(figsize=(10, 10))
                plt.scatter(city_data_df['Lat'], city_data_df['Humidity'], color='blue', edgecolor='black', alpha=0.75)
                
                # Add title and labels
                plt.title(f"City Latitude vs. Humidity ({time.strftime('%Y-%m-%d')})")
                plt.xlabel('Latitude')
                plt.ylabel('Humidity (%)')
                plt.grid(True)
                
                # Show plot
                plt.show()
                
                # Latitude Vs. Cloudiness
                # Create scatter plot
                plt.figure(figsize=(10, 10))
                plt.scatter(city_data_df['Lat'], city_data_df['Cloudiness'], color='blue', edgecolor='black', alpha=0.75)
                
                # Add title and labels
                plt.title(f"City Latitude vs. Cloudiness ({time.strftime('%Y-%m-%d')})")
                plt.xlabel('Latitude')
                plt.ylabel('Cloudiness (%)')
                plt.grid(True)
                
                # Show plot
                plt.show()
                
                # Latitude Vs. Wind Speed Plot
                # Create scatter plot
                plt.figure(figsize=(10, 10))
                plt.scatter(city_data_df['Lat'], city_data_df['Wind Speed'], color='blue', edgecolor='black', alpha=0.75)
                
                # Add title and labels
                plt.title(f"City Latitude vs. Wind Speed ({time.strftime('%Y-%m-%d')})")
                plt.xlabel('Latitude')
                plt.ylabel('Wind Speed (mph)')
                plt.grid(True)
                
                # Show plot
                plt.show()
                
                # Requirement 2: Compute Linear Regression for Each Relationship
                
                def plot_linear_regression(x_values, y_values, title, text_coordinates):
                    import matplotlib.pyplot as plt
                    from scipy.stats import linregress
                    
                    # Run linear regression on hemisphere weather data
                    (slope, intercept, r_value, p_value, std_err) = linregress(x_values, y_values)
                    
                    # Calculate the regression line "y values" from the slope and intercept
                    regress_values = x_values * slope + intercept
                    
                    # Get the equation of the line
                    line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
                    
                    # Create a scatter plot and plot the regression line
                    plt.figure(figsize=(10, 10))  # Adjust figure size
                    plt.scatter(x_values, y_values, color='blue', edgecolor='black', alpha=0.75)  # Match scatter plot style
                    plt.plot(x_values, regress_values, "r")  # Match regression line color
                    
                    # Annotate the text for the line equation
                    plt.annotate(line_eq, text_coordinates, fontsize=15, color="red")
                    
                    # Add title and labels
                    plt.title(title)
                    plt.xlabel('Latitude')
                    plt.ylabel(title)
                    
                    # Print the r-value
                    print(f"The r-value is: {r_value}")
                    
                    # Show plot
                    plt.show()
                
                # Create a DataFrame with the Northern Hemisphere data (Latitude >= 0)
                northern_hemi_df = city_data_df.loc[(city_data_df["Lat"] >= 0)]
                
                # Create a DataFrame with the Southern Hemisphere data (Latitude < 0)
                southern_hemi_df = city_data_df.loc[(city_data_df["Lat"] < 0)]
                
                # Temperture Vs. Latitude Linear Regression Plot
                # Linear regression on Northern Hemisphere
                x_values = northern_hemi_df['Lat']
                y_values = northern_hemi_df['Max Temp']
                
                # Call the function
                plot_linear_regression(x_values, y_values,
                                       'Max Temp (°F) vs. Latitude in Northern Hemisphere',(6,30))
                
                # Linear regression on Southern Hemisphere
                x_values = southern_hemi_df['Lat']
                y_values = southern_hemi_df['Max Temp']
                
                # Call the function
                plot_linear_regression(x_values, y_values,
                                       'Max Temp (°F) vs. Latitude in Southern Hemisphere',(-50,90))
                
                # Humidity Vs. Latitude Linear Regression Plot
                # Northern Hemisphere
                x_values = northern_hemi_df['Lat']
                y_values = northern_hemi_df['Humidity']
                
                # Call the function
                plot_linear_regression(x_values, y_values,
                                       'Humidity (%) vs. Latitude in Northern Hemisphere',(40,10))
                
                # Southern Hemisphere
                x_values = southern_hemi_df['Lat']
                y_values = southern_hemi_df['Humidity']
                
                # Call the function
                plot_linear_regression(x_values, y_values,
                                       'Humidity (%) vs. Latitude in Southern Hemisphere',(-50,15))
                
                # Cloudiness Vs. Latitude Linear Regression Plot
                # Northern Hemisphere
                x_values = northern_hemi_df['Lat']
                y_values = northern_hemi_df['Cloudiness']
                
                # Call the function
                plot_linear_regression(x_values, y_values,
                                       'Cloudiness (%) vs. Latitude in Northern Hemisphere',(40,10))
                
                # Southern Hemisphere
                x_values = southern_hemi_df['Lat']
                y_values = southern_hemi_df['Cloudiness']
                
                # Call the function
                plot_linear_regression(x_values, y_values,
                                       'Cloudiness (%) vs. Latitude in Southern Hemisphere',(-50,60))
                
                # Wind Speed Vs. Latitude Linear Regression Plot
                # Northern Hemisphere
                x_values = northern_hemi_df['Lat']
                y_values = northern_hemi_df['Wind Speed']
                
                # Call the function
                plot_linear_regression(x_values, y_values,
                                       'Wind Speed (mph) vs. Latitude in Northern Hemisphere',(40,35))
                
                # Southern Hemisphere
                x_values = southern_hemi_df['Lat']
                y_values = southern_hemi_df['Wind Speed']
                
                # Call the function
                plot_linear_regression(x_values, y_values,
                                       'Wind Speed (mph) vs. Latitude in Southern Hemisphere',(-50,35))
                plt.show()
              
                import hvplot.pandas
                import pandas as pd
                import requests
                
                # Import API key
                from api_keys import geoapify_key
                
                
                # Load the CSV file created in Part 1 into a Pandas DataFrame
                city_data_df = pd.read_csv("output_data/cities.csv")
                
                # Display sample data
                city_data_df.head()
                # Configure the map plot
                map1 = city_data_df.hvplot.points(
                    x="Lng",
                    y="Lat",
                    hover_cols=["City"],
                    geo=True,
                    tiles="OSM",
                    title="City Locations",
                    frame_width=800,
                    frame_height=600,
                    color="City",
                    size=70,
                    alpha=0.5
                )
                # Display the map
                map1
                
                
                # Narrow down cities that fit criteria and drop any results with null values
                filtered_cities = city_data_df.dropna() 
                
                filtered_cities = filtered_cities[(filtered_cities['Max Temp'] > 70) & (filtered_cities['Max Temp'] < 73)]
                
                # Drop any rows with null values
                filtered_cities = filtered_cities.dropna()
                
                # Display sample data
                filtered_cities
                # Use the Pandas copy function to create DataFrame called hotel_df to store the city, country, coordinates, and humidity
                hotel_df = filtered_cities[["City", "Country", "Lat", "Lng", "Humidity"]].copy()
                
                # Add an empty column, "Hotel Name," to the DataFrame so you can store the hotel found using the Geoapify API
                hotel_df["Hotel Name"] = ""
                
                # Display sample data
                hotel_df.head(20)
                
                
                # Set parameters to search for a hotel
                radius = 10000
                limit = 5
                params = {"categories": "accommodation.hotel", "limit": limit, "apiKey": geoapify_key}
                
                # Print a message to follow up the hotel search
                print("Starting hotel search")
                
                # Iterate through the hotel_df DataFrame
                for index, row in hotel_df.iterrows():
                    # get latitude, longitude from the DataFrame
                    latitude = row["Lat"]
                    longitude = row["Lng"]
                    
                    # Add filter and bias parameters with the current city's latitude and longitude to the params dictionary
                    params["filter"] = f'circle:{longitude},{latitude},{radius}'
                    params["bias"] = f'proximity:{longitude},{latitude}'
                    
                    # Set base URL
                    base_url = "https://api.geoapify.com/v2/places"
                
                
                    # Make and API request using the params dictionaty
                    name_address = requests.get(base_url, params=params)
                    
                    # Convert the API response to JSON format
                    name_address = name_address.json()
                    
                    # Grab the first hotel from the results and store the name in the hotel_df DataFrame
                    try:
                        hotel_df.loc[index, "Hotel Name"] = name_address["features"][0]["properties"]["name"]
                    except (KeyError, IndexError):
                        # If no hotel is found, set the hotel name as "No hotel found".
                        hotel_df.loc[index, "Hotel Name"] = "No hotel found"
                        
                    # Log the search results
                    print(f"{hotel_df.loc[index, 'City']} - nearest hotel: {hotel_df.loc[index, 'Hotel Name']}")
                    
                    
                
                # Display sample data
                hotel_df
                mp2 = hotel_df.hvplot.points(
                    x="Lng",
                    y="Lat",
                    hover_cols=["Country", "Hotel Name"],
                    geo=True,
                    tiles="OSM",
                    title="City Locations",
                    frame_width=800,
                    frame_height=600,
                    color="City",
                    size=70,
                    alpha=0.5
                )
                mp2
                </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">SQL</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Title: Data Engineering and Analysis Project<p></p>

                Introduction:<p></p>
                This project portfolio aims to showcase professional skills in the field of data engineering and analysis by simulating the role of a new data engineer at Pewlett Hackard. The portfolio is divided into three main parts: data modeling, data engineering, and data analysis, each focusing on different skills and tasks.
                <p></p>
                Project Objectives:<p></p>
                
                Utilize data modeling techniques to draw an Entity Relationship Diagram (ERD) based on the provided CSV files, providing guidance for database design.<p></p>
                Design and create appropriate table structures, defining relationships between tables.<p></p>
                Ensure the correctness of primary keys and foreign keys, and create tables in the correct order to handle foreign key constraints.<p></p>
                Import the provided CSV data into corresponding SQL tables, ensuring data integrity and consistency.<p></p>
                Execute a series of data analysis tasks using SQL queries, answering questions about employees and departments, including but not limited to employee information queries, conditional filtering, and statistical calculations.
                <p></p>
                Generate relevant data analysis reports for business decision-making and further analysis.<p></p>
                Project Contents:<p></p>
                
                Data Modeling:<p></p>
                
                Utilize tools like QuickDBD to draw an Entity Relationship Diagram (ERD) based on careful examination of data content and relationships, guiding database design.<p></p>
                Data Engineering:<p></p>
                
                Based on the data modeling results, create appropriate table structures and define relationships between tables.<p></p>
                Ensure the correctness of primary keys and foreign keys, and create tables in the correct order to handle foreign key constraints.<p></p>
                Import the provided CSV data into corresponding SQL tables, ensuring data integrity and consistency.<p></p>
                Data Analysis:<p></p>
                
                Use SQL queries to execute a series of data analysis tasks, answering questions about employees and departments, including but not limited to employee information queries, conditional filtering, and statistical calculations.<p></p>
                Generate relevant data analysis reports for business decision-making and further analysis.<p></p>
                Project Deliverables:<p></p>
                
                Complete data engineering process, including data modeling, table structure design, and data import.<p></p>
                Data analysis reports providing valuable insights and findings about employees and departments.<p></p>
                Opportunity for learning and practicing data engineering and analysis skills, laying a solid foundation for future success in data-related career paths.<p></p>
                Technology Stack:<p></p>
                
                Data Modeling: Tools like QuickDBD for drawing Entity Relationship Diagrams (ERDs).<p></p>
                Data Engineering: Utilization of SQL databases for table structure design and data import.<p></p>
                Data Analysis: Implementation of SQL query language for data analysis and report generation.<p></p>
                Conclusion:<p></p>
                By completing this project portfolio, professional skills and capabilities in the field of data engineering and analysis are demonstrated, laying a solid foundation for future success in data-related career paths.
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <img src="https://i.ibb.co/GcfWPFz/ezgif-1-e416a79f2d.gif" alt="Excel Showcase">
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code>
                CREATE TABLE "departments" (
                    "dept_no" VARCHAR   NOT NULL,
                    "dept_name" VARCHAR   NOT NULL,
                    CONSTRAINT "pk_departments" PRIMARY KEY (
                        "dept_no"
                     )
                );
                
                CREATE TABLE "dept_emp" (
                    "emp_no" INT NOT NULL,
                    "dept_no" VARCHAR NOT NULL,
                    CONSTRAINT "pk_dept_emp" PRIMARY KEY (
                        "emp_no", "dept_no"
                     )
                );
                
                
                CREATE TABLE "dept_manager" (
                    "dept_no" VARCHAR NOT NULL,
                    "emp_no" INT NOT NULL,
                    CONSTRAINT "pk_dept_manager" PRIMARY KEY (
                        "dept_no", "emp_no"
                     )
                );
                
                
                CREATE TABLE "employees" (
                    "emp_no" INT   NOT NULL,
                    "emp_title" VARCHAR   NOT NULL,
                    "birth_date" DATE   NOT NULL,
                    "first_name" VARCHAR   NOT NULL,
                    "last_name" VARCHAR   NOT NULL,
                    "sex" VARCHAR   NOT NULL,
                    "hire_date" DATE   NOT NULL,
                    CONSTRAINT "pk_employees" PRIMARY KEY (
                        "emp_no"
                     )
                );
                
                CREATE TABLE "salaries" (
                    "emp_no" INT   NOT NULL,
                    "salary" INT   NOT NULL,
                    CONSTRAINT "pk_salaries" PRIMARY KEY (
                        "emp_no"
                     )
                );
                
                CREATE TABLE "titles" (
                    "title_id" VARCHAR   NOT NULL,
                    "title" VARCHAR   NOT NULL,
                    CONSTRAINT "pk_titles" PRIMARY KEY (
                        "title_id"
                     )
                );
                
                ALTER TABLE "dept_emp" ADD CONSTRAINT "fk_dept_emp_emp_no" FOREIGN KEY("emp_no")
                REFERENCES "employees" ("emp_no");
                
                ALTER TABLE "dept_emp" ADD CONSTRAINT "fk_dept_emp_dept_no" FOREIGN KEY("dept_no")
                REFERENCES "departments" ("dept_no");
                
                ALTER TABLE "dept_manager" ADD CONSTRAINT "fk_dept_manager_emp_no" FOREIGN KEY("emp_no")
                REFERENCES "employees" ("emp_no");
                
                ALTER TABLE "dept_manager" ADD CONSTRAINT "fk_dept_manager_dept_no" FOREIGN KEY("dept_no")
                REFERENCES "departments" ("dept_no");
                
                ALTER TABLE "employees" ADD CONSTRAINT "fk_employees_emp_title" FOREIGN KEY("emp_title")
                REFERENCES "titles" ("title_id");
                
                ALTER TABLE "salaries" ADD CONSTRAINT "fk_salaries_emp_no" FOREIGN KEY("emp_no")
                REFERENCES "employees" ("emp_no");
                
                select *
                from departments
                
                select *
                from dept_emp
                
                select *
                from dept_manager
                
                select *
                from titles
                
                select *
                from employees
                
                select *
                from salaries
                
                
                -- 1,List the employee number, last name, first name, sex, and salary of each employee.
                
                SELECT
                    e.emp_no,
                    e.last_name,
                    e.first_name,
                    e.sex,
                    s.salary
                FROM
                    employees e
                JOIN
                    salaries s ON e.emp_no = s.emp_no;
                  
                  
                -- 2,List the first name, last name, and hire date for the employees who were hired in 1986.
                
                SELECT
                    first_name,
                    last_name,
                    hire_date
                FROM
                    employees
                WHERE
                    EXTRACT(YEAR FROM hire_date) = 1986;
                
                
                -- 3,List the manager of each department along with their department number, department name, employee number, last name, and first name.
                
                SELECT
                    d.dept_no,
                    d.dept_name,
                    dm.emp_no,
                    e.last_name,
                    e.first_name
                FROM
                    departments d
                JOIN
                    dept_manager dm ON d.dept_no = dm.dept_no
                JOIN
                    employees e ON dm.emp_no = e.emp_no;
                  
                  
                -- 4,List the department number for each employee along with that employee’s employee number, last name, first name, and department name.
                
                SELECT
                    e.emp_no,
                    e.last_name,
                    e.first_name,
                    de.dept_no,
                    d.dept_name
                FROM
                    employees e
                JOIN
                    dept_emp de ON e.emp_no = de.emp_no
                JOIN
                    departments d ON de.dept_no = d.dept_no;
                
                
                -- 5,List first name, last name, and sex of each employee whose first name is Hercules and whose last name begins with the letter B.
                
                SELECT
                    first_name,
                    last_name,
                    sex
                FROM
                    employees
                WHERE
                    first_name = 'Hercules'
                    AND last_name LIKE 'B%';
                  
                  
                -- 6,List each employee in the Sales department, including their employee number, last name, and first name.
                
                SELECT
                    e.emp_no,
                    e.last_name,
                    e.first_name,
                    d.dept_name
                FROM
                    employees e
                JOIN
                    dept_emp de ON e.emp_no = de.emp_no
                JOIN
                    departments d ON de.dept_no = d.dept_no
                WHERE
                    d.dept_name = 'Sales';
                
                
                -- 7,List each employee in the Sales and Development departments, including their employee number, last name, first name, and department name.
                
                SELECT
                    e.emp_no,
                    e.last_name,
                    e.first_name,
                    d.dept_name
                FROM
                    employees e
                JOIN
                    dept_emp de ON e.emp_no = de.emp_no
                JOIN
                    departments d ON de.dept_no = d.dept_no
                WHERE
                    d.dept_name IN ('Sales', 'Development');
                
                
                -- 8,List the frequency counts, in descending order, of all the employee last names (that is, how many employees share each last name).
                
                SELECT
                    last_name,
                    COUNT(last_name) AS frequency
                FROM
                    employees
                GROUP BY
                    last_name
                ORDER BY
                    frequency DESC, last_name;</code></pre></div>
          </div>
      </div>
  </div>
</div>



<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">SQL Alchemy</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>
                Project Title: Climate Data Analysis and Climate Application Design<p></p>
                
                Project Overview:<p></p>
                
                The project aims to analyze and explore climate data in the Honolulu, Hawaii area, and design a Flask-based climate application to provide users with information about the climate in that region. The project utilizes Python programming language, SQLAlchemy database tool, Pandas data processing library, and Matplotlib data visualization library to perform analysis on climate data, and Flask framework to build a simple and user-friendly web application.
                <p></p>
                Project Features:
                <p></p>
                Precipitation and Station Analysis:<p></p>
                Query precipitation and station data from the climate database, perform data statistics and visualization analysis, 
                and present the precipitation situation over the past year and related information about observation stations in the Honolulu area to users.<p></p>
                Design a set of simple and clear API interfaces based on completed data analysis, allowing users to access real-time climate data about Honolulu through these interfaces.<p></p>
                Project Details:<p></p>
                
                Data Analysis Section:<p></p>
                
                Connect to the SQLite database using SQLAlchemy and reflect data tables into classes.<p></p>
                Retrieve climate data through Python code and SQL queries, and process and analyze the data using Pandas.<p></p>
                Plot time series graphs of precipitation data and histograms of observed temperature at stations using Matplotlib to intuitively display the data.<p></p>
                Calculate and print statistical summaries of precipitation data and min, max, and average temperatures of station observations.<p></p>
                Climate Application Design Section:<p></p>
                
                Design a web application using the Flask framework, providing a set of API interfaces for users to access climate data.<p></p>
                Accessing the homepage allows users to retrieve all available API paths.<p></p>
                Provide interfaces to query precipitation data for the past year, obtain a list of stations, and retrieve temperature data observed at the most active stations.<p></p>
                Technology Stack:<p></p>
                
                Python<p></p>
                SQLAlchemy<p></p>
                Pandas<p></p>
                Matplotlib<p></p>
                Flask<p></p>
                Achievements:<p></p>
                
                The project has produced a fully functional, concise, and easy-to-use climate data analysis and application program. Users can obtain climate data for the Honolulu area through web pages or API interfaces, providing convenience for travel and daily life. Moreover, the project's code structure is clear, with detailed comments, making it easy to understand and maintain.
                <p></p>
                Future Outlook:<p></p>
                
                In the future, consider deploying the application to a cloud server to provide online access services, or further expanding functionality, 
                such as adding user authentication and personalized settings, to enhance user experience and the practicality of the application.<p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <img src="https://i.ibb.co/ssqYwsq/1.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/2dJVhzN/2.png" alt="Excel Showcase">
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code>from matplotlib import style
                style.use('fivethirtyeight')
                import matplotlib.pyplot as plt
                
                import numpy as np
                import pandas as pd
                import datetime as dt
                
                # Reflect Tables into SQLAlchemy ORM
                import sqlalchemy
                from sqlalchemy.ext.automap import automap_base
                from sqlalchemy.orm import Session
                from sqlalchemy import create_engine, func
                
                # Create engine to hawaii.sqlite
                engine = create_engine("sqlite:///../Resources/hawaii.sqlite")
                
                # Reflect an existing database into a new model
                Base = automap_base()
                # Reflect the tables
                Base.prepare(autoload_with=engine)
                
                # View all of the classes that automap found
                Base.classes.keys()
                
                # Save references to each table
                Measurement = Base.classes.measurement
                Station = Base.classes.station
                
                # Create our session (link) from Python to the DB
                session = Session(engine)
                
                # Exploratory Precipitation Analysis
                recent_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()
                recent_date
                
                recent_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()
                year_ago = dt.date(2017, 8, 23) - dt.timedelta(days=365)
                
                precip_data = session.query(Measurement.date, Measurement.prcp).\
                    filter(Measurement.date >= year_ago).\
                    order_by(Measurement.date).all()
                
                precip_df = pd.DataFrame(precip_data, columns=['Date', 'Precipitation'])
                
                precip_df.set_index(precip_df['Date'], inplace=True)
                
                # Use Pandas Plotting with Matplotlib to plot the data
                plt.figure(figsize=(10, 10))
                plt.bar(precip_df.index, precip_df['Precipitation'], width=5, color='skyblue', alpha=0.8, label='Precipitation')
                plt.title('Precipitation Over the Last 12 Months')
                plt.xlabel('Date')
                plt.ylabel('Precipitation (inches)')
                plt.xticks(rotation=45, ha='right')
                plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.MonthLocator(interval=1))
                plt.legend(loc='upper center')
                plt.tight_layout()
                plt.show()
                
                precip_df.describe()
                
                # Exploratory Station Analysis
                station_count = session.query(Measurement.station).distinct().count()
                station_count
                
                active_stations = session.query(Measurement.station, func.count(Measurement.station)).\
                    group_by(Measurement.station).\
                    order_by(func.count(Measurement.station).desc()).all()
                active_stations
                
                most_active = active_stations[0][0]
                lowest_temp = session.query(func.min(Measurement.tobs)).filter(Measurement.station == most_active).all()
                highest_temp = session.query(func.max(Measurement.tobs)).filter(Measurement.station == most_active).all()
                avg_temp = session.query(func.avg(Measurement.tobs)).filter(Measurement.station == most_active).all()
                print(f"The lowest temperature at {most_active} is {lowest_temp}")
                print(f"The highest temperature at {most_active} is {highest_temp}")
                print(f"The average temperature at {most_active} is {avg_temp}")
                
                temp_data = session.query(Measurement.tobs).\
                    filter(Measurement.station == most_active).\
                    filter(Measurement.date >= year_ago).\
                    order_by(Measurement.date).all()
                temp_df = pd.DataFrame(temp_data, columns=['tobs'])
                
                plt.figure(figsize=(10, 10))
                plt.hist(temp_df['tobs'], bins=12, color='skyblue', alpha=0.8)
                plt.title('Temperature Observation Data')
                plt.xlabel('Temperature (°F)')
                plt.ylabel('Frequency')
                plt.grid(True)
                plt.tight_layout()
                plt.show()
                
                # Close Session
                session.close()

                # Import the dependencies.
from flask import Flask, jsonify
import sqlalchemy
from sqlalchemy.ext.automap import automap_base
from sqlalchemy.orm import Session
from sqlalchemy import create_engine, func
import datetime as dt
import numpy as np

#################################################
# Database Setup
#################################################
engine = create_engine("sqlite:///../Resources/hawaii.sqlite")

# reflect an existing database into a new model
Base = automap_base()

# reflect the tables
Base.prepare(autoload_with=engine)

# Save references to each table
Measurement = Base.classes.measurement

Station = Base.classes.station

# Create our session (link) from Python to the DB
session = Session(engine)

#################################################
# Flask Setup
#################################################
app = Flask(__name__)


#################################################
# Flask Routes
#################################################
@app.route("/")
def welcome():
    """List all available api routes."""
    return (
        f"Available Routes:<br/>"
        f"/api/v1.0/precipitation<br/>"
        f"/api/v1.0/stations<br/>"
        f"/api/v1.0/tobs<br/>"
        f"/api/v1.0/&lt;start&gt;<br/>"
        f"/api/v1.0/&lt;start&gt;/&lt;end&gt;<br/>"
    )

@app.route("/api/v1.0/precipitation")
def precipitation():
    # Calculate the date 1 year ago from the last data point in the database
    one_year_ago = dt.date(2017, 8, 23) - dt.timedelta(days=365)

    # Query for the dates and precipitation from the last year
    results = session.query(Measurement.date, Measurement.prcp).\
        filter(Measurement.date >= one_year_ago).all()

    # Convert the query results to a dictionary using date as the key and prcp as the value
    precip_data = {date: prcp for date, prcp in results}

    # Return the JSON representation of your dictionary
    return jsonify(precip_data)

@app.route("/api/v1.0/stations")
def stations():
    # Query for the stations
    results = session.query(Station.station).all()

    # Convert the query results to a list
    stations = list(np.ravel(results))

    # Return a JSON list of stations from the dataset
    return jsonify(stations)

@app.route("/api/v1.0/tobs")
def tobs():
    # Calculate the date 1 year ago from the last data point in the database
    one_year_ago = dt.date(2017, 8, 23) - dt.timedelta(days=365)

    # Query for the dates and temperature observations from the last year
    results = session.query(Measurement.date, Measurement.tobs).\
        filter(Measurement.date >= one_year_ago).all()

    # Convert the query results to a list
    tobs = list(np.ravel(results))

    # Return a JSON list of Temperature Observations (tobs) for the previous year
    return jsonify(tobs)

@app.route("/api/v1.0/<start>")
def start(start):
    # Convert the start date from string to date format
    try:
        start_date = dt.datetime.strptime(start, '%Y-%m-%d').date()
    except ValueError:
        return jsonify({"error": "Date format should be YYYY-MM-DD"}), 400
    
    # Query for the minimum, maximum, and average temperatures from the start date
    results = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\
        filter(Measurement.date >= start_date).all()

    # Convert the query results to a list
    start_temps = list(np.ravel(results))

    # Return a JSON list of the minimum, maximum, and average temperatures
    return jsonify(start_temps)

@app.route("/api/v1.0/<start>/<end>")
def start_end(start, end):
    # Convert the start and end dates from string to date format
    try:
        start_date = dt.datetime.strptime(start, '%Y-%m-%d').date()
        end_date = dt.datetime.strptime(end, '%Y-%m-%d').date()
    except ValueError:
        return jsonify({"error": "Date format should be YYYY-MM-DD"}), 400

    # Query for the minimum, maximum, and average temperatures for a given start-end range
    results = session.query(
        func.min(Measurement.tobs),
        func.max(Measurement.tobs),
        func.avg(Measurement.tobs)
    ).filter(
        Measurement.date >= start_date,
        Measurement.date <= end_date
    ).all()

    # Convert the query results to a list
    start_end_temps = list(np.ravel(results))

    # Return a JSON list of the minimum, maximum, and average temperatures for a given start-end range
    return jsonify(start_end_temps)

if __name__ == '__main__':
    app.run(debug=True)
              </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Data Collection, Web Scraping</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Title: Mars Data Analysis and Visualization<p></p>

                Introduction:<p></p>
                The Mars Data Analysis and Visualization project aims to extract data from Mars-related websites using web scraping techniques and present insights through data analysis and visualization. The project consists of two parts: extracting titles and preview text from Mars news articles, and analyzing Martian weather data obtained from a temperature website.
                <p></p>
                Project Highlights:<p></p>
                
                Utilizing automated browsing tools and Beautiful Soup library for web data scraping and parsing.<p></p>
                Employing Python data processing libraries such as Pandas for data organization, analysis, and visualization.<p></p>
                Presenting Mars news dynamics and climate characteristics through data analysis and visualization.<p></p>
                Delivering project results in an interactive Jupyter Notebook format for easy comprehension and sharing.<p></p>
                Technical Details:<p></p>
                
                Part 1: Mars News Data Extraction<p></p>
                
                Using Splinter automated browsing tool to access Mars news websites and parsing the content with Beautiful Soup.<p></p>
                Extracting news titles and preview text, storing the results in Python dictionary lists, and displaying them in the Jupyter Notebook.<p></p>
                Optionally exporting the extracted data to JSON files.<p></p>
                Part 2: Mars Weather Data Analysis<p></p>
                
                Employing Splinter automated browsing tool to access the Mars temperature data website and parsing HTML tables with Beautiful Soup.<p></p>
                Organizing the extracted data into Pandas DataFrame, performing data type conversion and manipulation.<p></p>
                Utilizing Pandas functions to analyze the dataset, including calculating the number of months on Mars, 
                identifying the coldest and warmest months, and analyzing atmospheric pressure variations.<p></p>
                Visualizing analysis results using Matplotlib or other data visualization tools, such as bar charts and line plots.<p></p>
                Project Deliverables:<p></p>
                
                Showcasing Mars news hot topics and climate characteristics through data analysis and visualization.<p></p>
                Providing an interactive Jupyter Notebook containing the complete process of data scraping, processing, analysis, and visualization, along with explanations and conclusions.<p></p>
                The project results serve as valuable references for Mars exploration research and can be utilized in education and public outreach efforts.<p></p>
                Conclusion:<p></p>
                The Mars Data Analysis and Visualization project demonstrates the application of web scraping and data analysis techniques to gain insights 
                into Mars news and climate data. Through this project, we enhance our understanding of Martian dynamics and environment, 
                contributing valuable insights for future Mars exploration and research endeavors.<p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <img src="https://i.ibb.co/93b0KZ7/1.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/0XtsgRQ/2.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/Bcyqct3/3.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/pfk5HR2/4.png" alt="Excel Showcase">
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code># Import Splinter and BeautifulSoup
                from splinter import Browser
                from bs4 import BeautifulSoup as soup
                
                # Initialize browser
                browser = Browser('chrome')
                
                # Visit the Mars news site
                url = 'https://static.bc-edx.com/data/web/mars_news/index.html'
                browser.visit(url)
                
                # Create a Beautiful Soup object
                html = browser.html
                soup = soup(html, 'html.parser')
                
                # Extract titles and preview text
                titles = soup.find_all('div', class_='content_title')
                previews = soup.find_all('div', class_='article_teaser_body')
                
                # Create an empty list to store dictionaries
                articles = []
                
                # Loop through text elements and store in dictionaries
                for title, preview in zip(titles, previews):
                    article_dict = {
                        'title': title.get_text(strip=True),
                        'preview': preview.get_text(strip=True)
                    }
                    articles.append(article_dict)
                
                # Print the list
                articles
                
                # Quit browser
                browser.quit()

                # Import necessary libraries
from splinter import Browser
from bs4 import BeautifulSoup as soup
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd

# Create a browser instance
browser = Browser('chrome')

# Visit the website
url = "https://static.bc-edx.com/data/web/mars_facts/temperature.html"
browser.visit(url)

# Create a Beautiful Soup object
html = browser.html
soup = soup(html, 'html.parser')

# Scrape the table data
table_rows = soup.find('table').find_all('tr')[1:]
data_rows = []
for tr in table_rows:
    td_elements = tr.find_all('td')
    row_data = [td.text.strip() for td in td_elements]
    data_rows.append(row_data)

# Create a Pandas DataFrame
column_names = ['id', 'terrestrial_date', 'sol', 'ls', 'month', 'min_temp', 'pressure']
mars_temperature_df = pd.DataFrame(data_rows, columns=column_names)

# Convert data types
mars_temperature_df['id'] = mars_temperature_df['id'].astype(int)
mars_temperature_df['terrestrial_date'] = pd.to_datetime(mars_temperature_df['terrestrial_date'])
mars_temperature_df['sol'] = mars_temperature_df['sol'].astype(int)
mars_temperature_df['ls'] = mars_temperature_df['ls'].astype(int)
mars_temperature_df['month'] = mars_temperature_df['month'].astype(int)
mars_temperature_df['min_temp'] = mars_temperature_df['min_temp'].astype(float)
mars_temperature_df['pressure'] = mars_temperature_df['pressure'].astype(float)

# Analyze the data
num_months_on_mars = mars_temperature_df['month'].nunique()
num_sols_of_data = mars_temperature_df['sol'].nunique()
average_low_temp_by_month = mars_temperature_df.groupby('month')['min_temp'].mean()
average_pressure_by_month = mars_temperature_df.groupby('month')['pressure'].mean()
average_length_of_sol_in_earth_days = 1.027491252
number_of_sols = 669  
equivalent_earth_days = number_of_sols * average_length_of_sol_in_earth_days

# Plot the data
plt.figure(figsize=(10, 10))  
plt.bar(average_low_temp_by_month.index, average_low_temp_by_month.values, color='skyblue')
plt.title('Average Temperature by Month on Mars')
plt.xlabel('Month')
plt.ylabel('Temperature in Celsius')
plt.xticks(range(1, 13))  
plt.axhline(y=average_low_temp_by_month.min(), color='red', linestyle='--')
plt.axhline(y=average_low_temp_by_month.max(), color='red', linestyle='--')
plt.show()

plt.figure(figsize=(10, 10))  
plt.bar(average_pressure_by_month.index, average_pressure_by_month.values, color='lightgreen')
plt.title('Average Pressure by Month on Mars')
plt.xlabel('Month')
plt.ylabel('Pressure in Pa')
plt.xticks(range(1, 13)) 
plt.axhline(y=average_pressure_by_month.min(), color='red', linestyle='--')
plt.axhline(y=average_pressure_by_month.max(), color='red', linestyle='--') 
plt.show()

plt.figure(figsize=(10, 10))  
plt.plot(mars_temperature_df.index, mars_temperature_df['min_temp'], color='lightblue')
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
plt.title('Minimum Temperature Over Terrestrial Days')
plt.xlabel('Number of Terrestrial Days')
plt.ylabel('Minimum Temperature (Celsius)')
plt.gcf().autofmt_xdate()
plt.grid(True)
plt.show()

# Save the data to a CSV file
mars_temperature_df.to_csv('mars_temperature_data.csv', index=True)

# Quit the browser
browser.quit()

                </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">NoSQL Databases, MongoDB</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Title: UK Food Hygiene Rating Data Analysis<p></p>
                Project Overview:<p></p>
                
                In a data-driven initiative for "Eat Safe, Love," a prominent UK food magazine, we executed a comprehensive analysis of food hygiene ratings across various establishments in the United Kingdom. The goal was to empower the magazine's editorial team with actionable insights, enabling them to spotlight high-quality eateries and inform their readership accurately about food safety standards.
                <p></p>
                Technical Summary:<p></p>
                
                Data Management: Utilized MongoDB, a NoSQL database, to manage a large dataset of food hygiene ratings, offering flexibility in handling diverse data types and structures.<p></p>
                Data Importation & Integration: Developed an automated pipeline to import the establishments.json file into the MongoDB database, ensuring data consistency and accuracy.<p></p>
                Database Customization: Tailored the database according to editorial specifications, including the integration of newly opened establishments and the refinement of existing data entries.<p></p>
                Data Transformation: Implemented data cleaning procedures using MongoDB's update_many method to standardize latitude, longitude, and rating values into their appropriate numeric formats.<p></p>
                Analytical Querying: Conducted targeted queries to isolate establishments based on hygiene scores, geographical location, and authority areas. Utilized MongoDB’s querying capabilities,
                 including regular expressions and aggregation frameworks.<p></p>
                Data Presentation: Transformed raw query outputs into structured data using Pandas DataFrames, which allowed for a clear presentation of findings, including sorting and filtering for top results.<p></p>
                Geospatial Analysis: Performed proximity searches to compare establishment locations, pinpointing those within a specified range of a newly added restaurant for competitive analysis.<p></p>
                Key Insights:<p></p>
                
                Identified establishments with exceptional hygiene scores, enabling the magazine to feature the top performers in upcoming publications.<p></p>
                Analyzed the distribution of hygiene scores across London, revealing patterns and trends that could influence future editorial directions.<p></p>
                Utilized geospatial data to locate top-rated establishments in close vicinity to "Penang Flavours," a new restaurant of interest, to explore the competitive landscape.<p></p>
                Provided a comprehensive overview of hygiene standards across local authorities, assisting the magazine in planning regional focus areas for content.<p></p>
                Outcome:<p></p>
                
                The project culminated in a robust analytical report, complete with visualizations and strategic recommendations, to guide "Eat Safe, Love" in their mission to celebrate high-quality dining experiences and promote food safety awareness. The insights garnered from the analysis have not only influenced editorial content but also enriched the magazine's narrative on the food industry's compliance with hygiene standards.
                <p></p>
                Technologies Used:<p></p>
                
                MongoDB<p></p>
                PyMongo<p></p>
                Python<p></p>
                Pandas<p></p>
                Jupyter Notebook<p></p>
                Git and GitHub<p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRltTA4JDfYMru1oWKLHuhTez-syAOT1UoqG55RkEcW&s" alt="Excel Showcase">
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code># Import dependencies
                from pymongo import MongoClient
                from pprint import pprint
                
                # Create an instance of MongoClient
                mongo = MongoClient(port=27017)
                
                # confirm that our new database was created
                print(mongo.list_database_names())
                
                # assign the uk_food database to a variable name
                db = mongo['uk_food']
                
                # review the collections in our new database
                print(db.list_collection_names())
                
                # review a document in the establishments collection
                pprint(db.establishments.find_one())
                
                # assign the collection to a variable
                establishments = db['establishments']
                
                # Add a new restaurant "Penang Flavours" to the database
                new_restaurant = {
                    "BusinessName":"Penang Flavours",
                    "BusinessType":"Restaurant/Cafe/Canteen",
                    "BusinessTypeID":"",
                    "AddressLine1":"Penang Flavours",
                    "AddressLine2":"146A Plumstead Rd",
                    "AddressLine3":"London",
                    "AddressLine4":"",
                    "PostCode":"SE18 7DY",
                    "Phone":"",
                    "LocalAuthorityCode":"511",
                    "LocalAuthorityName":"Greenwich",
                    "LocalAuthorityWebSite":"http://www.royalgreenwich.gov.uk",
                    "LocalAuthorityEmailAddress":"health@royalgreenwich.gov.uk",
                    "scores":{
                        "Hygiene":"",
                        "Structural":"",
                        "ConfidenceInManagement":""
                    },
                    "SchemeType":"FHRS",
                    "geocode":{
                        "longitude":"0.08384000",
                        "latitude":"51.49014200"
                    },
                    "RightToReply":"",
                    "Distance":4623.9723280747176,
                    "NewRatingPending":True
                }
                establishments.insert_one(new_restaurant)
                
                # Find the BusinessTypeID for "Restaurant/Cafe/Canteen"
                business_type = establishments.find_one({"BusinessType": "Restaurant/Cafe/Canteen"}, {"BusinessTypeID": 1, "BusinessType": 1})
                print(business_type)
                
                # Update the new restaurant with the correct BusinessTypeID
                establishments.update_one({"BusinessName": "Penang Flavours"}, {"$set": {"BusinessTypeID": business_type['BusinessTypeID']}})
                
                # Confirm that the new restaurant was updated
                updated_restaurant = establishments.find_one({"BusinessName": "Penang Flavours"})
                print(updated_restaurant)
                
                # Delete all documents where LocalAuthorityName is "Dover"
                establishments.delete_many({"LocalAuthorityName": "Dover"})
                
                # Check if any remaining documents include Dover
                dover_count_after_deletion = establishments.count_documents({"LocalAuthorityName": "Dover"})
                print(dover_count_after_deletion)
                
                # Check that other documents remain with 'find_one'
                other = establishments.find_one({"LocalAuthorityName": "Greenwich"})
                print(other)
                
                # Change the data type from String to Decimal for longitude and latitude
                establishments.update_many(
                    {},
                    [
                        {"$set": {"geocode.longitude": {"$toDecimal": "$geocode.longitude"}}},
                        {"$set": {"geocode.latitude": {"$toDecimal": "$geocode.latitude"}}}
                    ]
                )
                
                # Set non 1-5 Rating Values to Null
                non_ratings = ["AwaitingInspection", "Awaiting Inspection", "AwaitingPublication", "Pass", "Exempt"]
                establishments.update_many({"RatingValue": {"$in": non_ratings}}, [ {'$set':{ "RatingValue" : None}} ])
                
                # Change the data type from String to Integer for RatingValue
                establishments.update_many(
                    {},
                    [
                        { "$set": { "RatingValue": { "$toInt": "$RatingValue" } } }
                    ]
                )
                
                # Check that the coordinates and rating value are now numbers
                document = establishments.find_one()
                
                longitude_type = type(document['geocode']['longitude'])
                latitude_type = type(document['geocode']['latitude'])
                rating_value_type = type(document['RatingValue'])
                
                print(f"Longitude type: {longitude_type}")
                print(f"Latitude type: {latitude_type}")
                print(f"RatingValue type: {rating_value_type}")

                # Set Up
from pymongo import MongoClient
from pprint import pprint
import pandas as pd
from bson.decimal128 import Decimal128

# Connect to MongoDB on the default port
mongo = MongoClient(port=27017)

# Select the 'uk_food' database
db = mongo['uk_food']

# Get a list of all collections in the database
print(db.list_collection_names())

# Select the 'establishments' collection
establishments = db['establishments']

# Part 3: Exploratory Analysis

# Question 1: Find establishments with a hygiene score of 20
query = {"scores.Hygiene": 20}
count = establishments.count_documents(query)
first_document = establishments.find_one(query)
pprint(first_document)

# Convert to DataFrame and print results
docs = establishments.find(query)
df = pd.DataFrame(list(docs))
print("Number of rows in the DataFrame:", len(df))
print(df.head(10))

# Question 2: Find London establishments with a RatingValue of 4 or more
query = {
    "LocalAuthorityName": {"$regex": "London", "$options": "i"}, 
    "RatingValue": {"$gte": 4}
}
count = establishments.count_documents(query)
first_doc = establishments.find_one(query)
pprint(first_doc)

# Convert to DataFrame and print results
docs = establishments.find(query)
df = pd.DataFrame(list(docs))
print("Number of rows in the DataFrame:", len(df))
print(df.head(10))

# Question 3: Top 5 establishments with a RatingValue of 5 by hygiene, near 'Penang Flavours'
restaurant = establishments.find_one({"BusinessName": "Penang Flavours"})
if restaurant:
    degree_search = Decimal128('0.01')
    latitude = restaurant["geocode"]["latitude"]
    longitude = restaurant["geocode"]["longitude"]
    query = {
        "RatingValue": 5,
        "geocode.latitude": {"$gte": Decimal128(latitude.to_decimal() - degree_search.to_decimal()),
                             "$lte": Decimal128(latitude.to_decimal() + degree_search.to_decimal())},
        "geocode.longitude": {"$gte": Decimal128(longitude.to_decimal() - degree_search.to_decimal()),
                              "$lte": Decimal128(longitude.to_decimal() + degree_search.to_decimal())}
    }
    sort = [("scores.Hygiene", 1)]
    results = establishments.find(query).sort(sort)
    for result in results:
        print(result["BusinessName"], result["scores"]["Hygiene"])
else:
    print("Restaurant 'Penang Flavours' not found.")

# Convert to DataFrame
docs = establishments.find(query).sort(sort)
df = pd.DataFrame(list(docs))

# Question 4: Count of establishments by Local Authority with a hygiene score of 0
pipeline = [
    {"$match": {"scores.Hygiene": 0}},
    {"$group": {"_id": "$LocalAuthorityName", "count": {"$sum": 1}}},
    {"$sort": {"count": -1}}
]
results = list(establishments.aggregate(pipeline))
print(f"Number of documents found: {len(results)}")
for result in results[:10]:
    print(f"Local Authority: {result['_id']}, count: {result['count']}")

# Convert to DataFrame and print results
df = pd.DataFrame(results)
print("Number of rows in the DataFrame:", len(df))
print(df.head(10))

                </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Interactive Visualizations</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>
                Project Title: Belly Button Biodiversity Dashboard<p></p>
                
                Introduction:<p></p>
                The Belly Button Biodiversity Dashboard project aims to create an interactive web application to explore and visualize the Belly Button Biodiversity dataset, which catalogs the microbes that colonize human navels. The dataset provides insights into the diversity of microbial species present in human navels, with some species being more prevalent than others.
                <p></p>
                Project Objectives:<p></p>
                
                Build an interactive dashboard using D3 library to visualize the Belly Button Biodiversity dataset.<p></p>
                Create a horizontal bar chart to display the top 10 operational taxonomic units (OTUs) found in each individual's navel.<p></p>
                Generate a bubble chart to visualize each sample, with the size of the markers representing the sample values and the color representing the OTU IDs.<p></p>
                Display demographic information for each individual, extracted from the sample metadata.<p></p>
                Update all plots dynamically when a new sample is selected from the dropdown menu.<p></p>
                Deploy the dashboard to a free static page hosting service, such as GitHub Pages.<p></p>
                Project Features:<p></p>
                
                Data Visualization: Utilize D3 library to create interactive and visually appealing charts.<p></p>
                Interactivity: Enable users to explore the dataset by selecting different samples and viewing corresponding visualizations.<p></p>
                Dynamic Updates: Ensure that all plots are updated in real-time when a new sample is selected.<p></p>
                User-friendly Interface: Design an intuitive and aesthetically pleasing dashboard layout for easy navigation and comprehension.<p></p>
                Deployment: Deploy the dashboard to GitHub Pages for easy access and sharing.<p></p>
                Advanced Challenge (Optional):<p></p>
                For those seeking an extra challenge, adapt the Gauge Chart to plot the weekly washing frequency of the individual. Modify the gauge chart code to account for values ranging from 0 through 9 and update the chart dynamically based on the selected sample.
                <p></p>
                Project Deliverables:<p></p>
                
                Interactive Dashboard: Includes horizontal bar chart, bubble chart, and sample metadata display.<p></p>
                Codebase: JavaScript code utilizing D3 library for data visualization and interactivity.<p></p>
                README.md: Provides project documentation, including setup instructions, project overview, and any additional information.<p></p>
                GitHub Repository: Contains all project files and codebase with regular commits.<p></p>
                Deployment: Dashboard deployed to GitHub Pages with the corresponding link provided.<p></p>
                
                Through the Belly Button Biodiversity Dashboard project, users can gain insights into the microbial diversity of 
                human navels and explore the dataset in an interactive and engaging manner.<p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <img src="https://i.ibb.co/kgYWkYC/1.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/CBWx8Z7/2.png" alt="Excel Showcase">
                
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code>
// Function to set things up initially
function init() {
    // Pick the dropdown menu
    var dropdownMenu = d3.select("#selDataset");
  
    // Get data from a URL and fill the dropdown with sample IDs
    d3.json("https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.1/14-Interactive-Web-Visualizations/02-Homework/samples.json").then(data => {
      data.names.forEach(name => {
        dropdownMenu.append("option").text(name).property("value", name);
      });
  
      // Start with the first sample to make initial charts
      const firstSample = data.names[0];
      buildCharts(firstSample);
      updateDemographicInfo(firstSample);
    });
  }
  
  // Function to create charts for a specific sample
  function buildCharts(sample) {
    // Get data from a URL
    d3.json("https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.1/14-Interactive-Web-Visualizations/02-Homework/samples.json").then(data => {
      // Filter data for the chosen sample
      var sampleData = data.samples.filter(obj => obj.id == sample)[0];
  
      // BAR CHART
      // Sort and slice data for a bar chart
      var topSampleValues = sampleData.sample_values.slice(0, 10).reverse();
      var topOtuIds = sampleData.otu_ids.slice(0, 10).reverse();
      var topOtuLabels = sampleData.otu_labels.slice(0, 10).reverse();
  
      // Create data for the bar chart
      var barTrace = {
        x: topSampleValues,
        y: topOtuIds.map(otuID => `OTU ${otuID}`),
        text: topOtuLabels,
        type: "bar",
        orientation: "h"
      };
  
      var barData = [barTrace];
  
      var barLayout = {
        title: "Top 10 OTUs Found",
        margin: { t: 30, l: 150 }
      };
  
      // Make the bar chart
      Plotly.newPlot("bar", barData, barLayout);
  
      // BUBBLE CHART
      // Create data for a bubble chart
      var bubbleTrace = {
        x: sampleData.otu_ids,
        y: sampleData.sample_values,
        text: sampleData.otu_labels,
        mode: 'markers',
        marker: {
          size: sampleData.sample_values,
          color: sampleData.otu_ids,
          colorscale: 'Earth'
        }
      };
  
      var bubbleData = [bubbleTrace];
  
      var bubbleLayout = {
        title: 'Bacteria Cultures Per Sample',
        showlegend: false,
        height: 600,
        width: 1200,
        xaxis: { title: 'OTU ID' }
      };
  
      // Make the bubble chart
      Plotly.newPlot('bubble', bubbleData, bubbleLayout);
    });
  }
  
  // Function to update demographic info for a specific sample
  function updateDemographicInfo(sample) {
    // Choose the demographic info panel
    var demographicInfoPanel = d3.select("#sample-metadata");
  
    // Clear the current content
    demographicInfoPanel.html("");
  
    // Get data from a URL
    d3.json("https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.1/14-Interactive-Web-Visualizations/02-Homework/samples.json").then(data => {
      // Filter data for the chosen sample's metadata
      var metadata = data.metadata.filter(obj => obj.id == sample)[0];
  
      // Display demographic info
      Object.entries(metadata).forEach(([key, value]) => {
        demographicInfoPanel.append("h6").text(`${key}: ${value}`);
      });
    });
  }
  
  // Function to handle selecting a new sample
  function optionChanged(newSample) {
    // Update the charts
    buildCharts(newSample);
    // Update the demographic info
    updateDemographicInfo(newSample);
  }
  
  // Set up the dashboard when the page loads
  init();
              </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Mapping</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Title: USGS Earthquake Data Visualization<p></p>

                Introduction:<p></p>
                The United States Geological Survey (USGS) is responsible for providing scientific data regarding natural hazards, ecosystem health, and the impacts of climate and land-use change. Their scientists develop innovative methods and tools to deliver timely, relevant, and useful information about the Earth and its processes.
                <p></p>
                The USGS is interested in developing new tools to visualize their earthquake data. Despite collecting a vast amount of data worldwide every day, they lack a meaningful way to present it. In this challenge, you are tasked with creating a visualization method for USGS data to better educate the public and other government organizations and, hopefully, secure additional funding for addressing global issues.
                <p></p>
                Project Objectives:<p></p>
                
                Utilize Leaflet library to create earthquake data visualization.<p></p>
                Plot data markers on the map based on earthquake longitude and latitude.<p></p>
                Adjust the size of data markers based on earthquake magnitude and color based on earthquake depth.<p></p>
                Create a legend to provide context for the map data.<p></p>
                Challenge Tasks:<p></p>
                
                Retrieve Earthquake Data:<p></p>
                
                Visit the USGS GeoJSON Feed page and select an earthquake dataset for visualization.<p></p>
                Obtain the data using the JSON URL of the selected dataset.<p></p>
                Import and Visualize Data:<p></p>
                
                Create a map using the Leaflet library and plot all earthquakes' data markers based on their longitude and latitude.<p></p>
                Adjust the size of data markers based on earthquake magnitude and color based on earthquake depth.<p></p>
                Include popups with additional earthquake information when markers are clicked.<p></p>
                Create a legend to provide context for the map data.<p></p>
                Gather and Plot More Data (Optional):<p></p>
                
                Plot tectonic plate data on the map to illustrate the relationship between tectonic plates and seismic activity.<p></p>
                Display tectonic plate data alongside the original earthquake data visualization.<p></p>
                Organize earthquake data and tectonic plate data into separate overlays that can be toggled independently.<p></p>
                Add layer controls to the map.<p></p>
                
                Through this project, you will demonstrate how to create interactive maps using the Leaflet library to visually represent 
                earthquake data. Additionally, you will plot tectonic plate data on the map, providing crucial information about Earth's activity to the public and governmental organizations.<p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <img src="https://i.ibb.co/cCyMNTt/1.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/GsVQGt7/4.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/PrcmQ8k/2.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/HqF6rfX/3.png" alt="Excel Showcase">
                
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code>
                // Initialize the map
                const myMap = L.map("map", {
                  center: [37.8, -96],
                  zoom: 4,
                });
                
                // Base layers
                const esriMap = L.tileLayer('https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}', {
                  attribution: 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'
                });
                const openStreetMap = L.tileLayer("https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png", {
                  attribution: "&copy; <a href='https://www.openstreetmap.org/copyright'>OpenStreetMap</a> contributors",
                });
                
                // Add the Esri base layer to the map
                esriMap.addTo(myMap);
                
                // Define the color for each depth range
                function getColor(d) {
                  return d > 90 ? '#08306b' :
                         d > 70 ? '#08519c' :
                         d > 50 ? '#2171b5' :
                         d > 30 ? '#4292c6' :
                         d > 10 ? '#6baed6' :
                                  '#9ecae1';
                }
                
                
                // Marker styling functions
                const markerSize = magnitude => magnitude * 5;
                const markerColor = depth => getColor(depth); // Use the getColor function for depth color
                
                // Overlay map object for layer control
                const overlayMaps = {};
                
                // Legend
                const legend = L.control({ position: "bottomright" });
                legend.onAdd = () => {
                  const div = L.DomUtil.create("div", "info legend");
                  const grades = [-10, 10, 30, 50, 70, 90]; // Depth ranges
                  let labels = ['<strong>Depth (km)</strong>'];
                
                  // Generate a label with a colored square for each depth range
                  grades.forEach((grade, index) => {
                    labels.push(
                      `<i style="background: ${getColor(grade)}"></i> ${grade}${grades[index + 1] ? '&ndash;' + grades[index + 1] : '+'} km`
                    );
                  });
                
                  div.innerHTML = labels.join('<br>');
                  return div;
                };
                legend.addTo(myMap);
                
                // Fetch and add earthquake data
                $.getJSON("https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_week.geojson", geoJsonData => {
                  const earthquakes = L.geoJSON(geoJsonData, {
                    pointToLayer: (feature, latlng) => L.circleMarker(latlng, {
                      radius: markerSize(feature.properties.mag),
                      fillColor: markerColor(feature.geometry.coordinates[2]),
                      color: "#000",
                      weight: 1,
                      opacity: 1,
                      fillOpacity: 0.8, // Semi-transparent
                    }),
                    onEachFeature: (feature, layer) => layer.bindPopup(`Location: ${feature.properties.place}<br>Magnitude: ${feature.properties.mag}<br>Depth: ${feature.geometry.coordinates[2]} km`),
                  });
                
                  overlayMaps.Earthquakes = earthquakes;
                  earthquakes.addTo(myMap);
                });
                
                // Base maps object for layer control
                const baseMaps = {
                  "Esri World Street Map": esriMap,
                  "OpenStreetMap": openStreetMap
                };
                
                // Layer control to allow user selection of base map and overlay
                L.control.layers(baseMaps, overlayMaps, { collapsed: false }).addTo(myMap)
                </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Tableau</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Title: Citi Bike Data Analysis and Visualization<p></p>

                Project Overview:<p></p>
                The Citi Bike Data Analysis and Visualization project aims to provide a comprehensive understanding of the New York City Citi Bike bike-sharing program and offer valuable insights to decision-makers. By analyzing and visualizing data from the Citi Bike Trip History Logs, we will uncover key trends, user behaviors, and potential issues to support urban planning and transportation management.
                <p></p>
                Project Objectives:
                <p></p>
                Discover Unexpected Phenomena: Identify and explain two unexpected phenomena through data analysis, such as sudden increases in ride volume or unusual activity at specific stations.<p></p>
                Visualization Design: Design 2-5 visualizations to answer questions and showcase key metrics and trends of the project, including ride volume, user types, ride duration, station popularity, and more.<p></p>
                Map Creation: Create basic or advanced maps to illustrate the popular locations of bike stations and their changing trends over time, along with relevant geographic information.<p></p>
                Storytelling: Utilize Tableau Story to integrate all visualizations and explanatory analysis into a cohesive narrative for presenting to stakeholders and the public.<p></p>
                Final Presentation: Share the project via Tableau Public, ensuring readability, visual appeal, and professionalism in the report and visualizations.<p></p>
                Project Value:<p></p>
                
                Provide the New York City government with an in-depth understanding of the Citi Bike program and data-supported insights to improve urban transportation planning and management.<p></p>
                Showcase the impact and value of the bike-sharing program to the public, encouraging more sustainable modes of transportation.<p></p>
                Demonstrate data analysis and visualization skills, highlighting professionalism and insightfulness to support future career development.<p></p>
                The project will be presented with professionally designed reports and visualizations, offering clear, intuitive data insights while emphasizing the i
                mportance and influence of the Citi Bike program.<p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <p>The graph shows a clear seasonal pattern in ride counts, with a significant peak in July and a sharp drop towards November. The total ride duration has a similar trend, indicating higher usage in warmer months. These patterns likely reflect increased recreational or tourism-related bike usage during summer and a decrease as temperatures drop.<p></p>
                <img src="https://i.ibb.co/xMLKKm9/1.png" alt="Excel Showcase">
                <p>The pie chart shows the distribution between two types of users: 'Customers' and 'Subscribers'. Subscribers form the vast majority with 87.56% of total rides, amounting to 1,665,722 rides. Customers, who are likely casual or one-time users, represent a smaller portion, 12.44%, which equates to 236,746 rides. This distribution suggests a strong base of regular users.<p></p>
                <img src="https://i.ibb.co/St1T8xF/2.png" alt="Excel Showcase">
                <p>
                  The pie chart represents the gender distribution of bike ride users. It shows that 67.32% of the rides were taken by males (labeled as '1'), 20.23% by females (labeled as '2'), and 12.45% of the rides are attributed to users with unknown gender (labeled as '0'). This data is out of a total of 1,902,468 recorded rides. The large majority of known gender riders are male.<p></p>
                <img src="https://i.ibb.co/LCkLHhG/6.png" alt="Excel Showcase">
                <p>The histogram displays ride counts by hour across different days of the week. Each day is color-coded: Sunday (blue), Monday (orange), Tuesday (red), Wednesday (green), Thursday (light blue), Friday (yellow), Saturday (purple). Peaks in ride counts are during morning and evening hours on weekdays, suggesting commuter activity. Weekends show a more even distribution, peaking midday, indicative of leisure activities. This pattern suggests that riders use the service for work on weekdays and for recreation on weekends.<p></p>
                <img src="https://i.ibb.co/QmnB7Lv/3.png" alt="Excel Showcase">
                <p>This visual consists of two parts. The map highlights Jersey City with numerous bike-share stations, densest in downtown areas. The bar chart ranks these stations by popularity based on the number of rides started at each location, with the most popular stations having the most significant number of starts. Both tools likely serve to analyze and improve urban bike-sharing systems.<p></p>
                <img src="https://i.ibb.co/kJkhfks/4.png" alt="Excel Showcase">
                <p>This visual features a map and bar chart for a bike-sharing system in Jersey City. The map pinpoints the end stations for bike rides, with the size of each dot reflecting the volume of rides ending at that station. The largest clusters are around central Jersey City. The bar chart below the map ranks the end stations in terms of popularity based on ride count, with the longest bars indicating the most frequented stations. The top stations have significantly more activity, suggesting they are key destinations for riders.<p></p>
                <img src="https://i.ibb.co/hR7qMLC/5.png" alt="Excel Showcase">
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Clustering of Unsupervised Learning</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Title: Cryptocurrency Market Data Analysis and Clustering<p></p>
                Project Overview:<p></p>
                This project aims to analyze and cluster cryptocurrency market data using data analysis and machine learning techniques to uncover potential patterns and trends in the cryptocurrency market. Through techniques such as elbow method analysis, K-means clustering, and Principal Component Analysis (PCA), the project preprocesses, clusters, and reduces the dimensionality of cryptocurrency market data to gain deeper insights into the characteristics and dynamics of the cryptocurrency market.
                <p></p>
                Data Source:<p></p>
                The project utilizes a sample dataset of cryptocurrency market data, including price variations of different cryptocurrencies and other market-related metrics.
                <p></p>
                Technologies and Methods:<p></p>
                Data Preprocessing: The original data is standardized to ensure that the data has similar scales and ranges.<p></p>
                Elbow Method Analysis: By plotting the elbow curve, the optimal number of clusters is selected to optimize the performance of the K-means clustering algorithm.<p></p>
                K-means Clustering: K-means clustering algorithm is applied to the cryptocurrency market data to identify similarities and differences between different cryptocurrencies.<p></p>
                Principal Component Analysis (PCA): PCA is used to reduce the dimensionality of the data to three principal components, reducing data complexity while retaining most of the information.<p></p>
                Next Steps:<p></p>
                Moving forward, the project will include, but not limited to:<p></p>
                
                Selecting appropriate machine learning models, such as regression models or classification models, to predict cryptocurrency price trends.<p></p>
                Conducting feature engineering, including feature selection, handling missing values, and outlier detection.<p></p>
                Training and evaluating models to ensure good predictive performance and generalization ability.<p></p>
                Interpreting models and visualizing results to better understand the characteristics and dynamics of the cryptocurrency market.<p></p>
                Conclusion:<p></p>
                Through preliminary analysis and clustering in this project, we have gained initial insights into the characteristics and behaviors of the cryptocurrency market, laying the foundation for further data analysis and modeling work. By continuously exploring and analyzing cryptocurrency market data, we will be able to better understand market changes and trends, and make informed decisions and predictions.
                <p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <p>This line chart illustrates the price change percentage of different cryptocurrencies over various time frames, ranging from 24 hours to 1 year. Each line represents a different time frame, revealing the volatility and price dynamics of each coin. The spike in one of the coins suggests an extreme price change within a specific period. This visualization helps in assessing the short-term and long-term performance and stability of each cryptocurrency.<p></p>
                <img src="https://i.ibb.co/dmYsRLx/1.png" alt="Excel Showcase">
                <p>
                  This Elbow Curve graph is used to determine the optimal number of clusters for KMeans clustering. It plots the inertia (within-cluster sum of squares) against the number of clusters. The 'elbow' point in the curve indicates the number where adding more clusters doesn't significantly improve the fit. Here, the elbow seems to be around the cluster count of 4, suggesting that it's the optimal number of clusters for this particular dataset.<p></p>
                <img src="https://i.ibb.co/RTqJ1Jn/2.png" alt="Excel Showcase">
                <p>
                  This scatter plot visualizes the clustering results of cryptocurrencies based on their 24-hour and 7-day price changes. Each point represents a cryptocurrency, and the colors represent different clusters identified by the KMeans algorithm. The plot shows how cryptocurrencies are grouped together based on the similarity of their price fluctuation metrics over the specified periods, indicating potential patterns or correlations in their price movements.<p></p>
                <img src="https://i.ibb.co/JK0Tkfg/3.png" alt="Excel Showcase">
                <p>
                  This scatter plot is a visual representation of cryptocurrencies after dimensionality reduction using PCA, which has transformed the data into 3 principal components. Here, only the first two principal components, PC1 and PC2, are plotted. Each point corresponds to a cryptocurrency and is colored based on the cluster it belongs to, as identified by the KMeans algorithm post-PCA. The spread of points along the PC1 and PC2 axes shows how the cryptocurrencies differ from each other based on the principal components derived from their original features.<p></p>
                <img src="https://i.ibb.co/wWpzpmQ/4.png" alt="Excel Showcase">
                <p>This image shows a composite plot of two Elbow Curves, designed to compare the inertia values from two different datasets or clustering approaches. The chart has two curves, one in blue and the other in red, both declining as the number of clusters increases. This composite visualization helps in determining the optimal number of clusters (k) for KMeans clustering by identifying the point where the rate of decrease in inertia significantly slows down, which appears to be at 4 clusters for both curves.<p></p>
                <img src="https://i.ibb.co/GpkK0BZ/5.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/xg3Sc0V/6.png" alt="Excel Showcase">
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code>import pandas as pd
                import hvplot.pandas
                from sklearn.cluster import KMeans
                from sklearn.decomposition import PCA
                from sklearn.preprocessing import StandardScaler
                
                df_market_data = pd.read_csv("Resources/crypto_market_data.csv", index_col="coin_id")
                print(df_market_data.head(10))
                print(df_market_data.describe())
                df_market_data.hvplot.line(width=1000, height=600, rot=90)
                
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(df_market_data)
                
                df_market_data_scaled = pd.DataFrame(scaled_data, columns=df_market_data.columns)
                df_market_data_scaled["coinid"] = df_market_data.index
                df_market_data_scaled.set_index("coinid", inplace=True)
                print(df_market_data_scaled.head(10))
                
                k = list(range(1, 11))
                inertia = []
                for i in k:
                    model = KMeans(n_clusters=i, random_state=0)
                    model.fit(df_market_data_scaled)
                    inertia.append(model.inertia_)
                
                elbow_data = {"k": k, "inertia": inertia}
                df_elbow = pd.DataFrame(elbow_data)
                
                elbow_plot_1 = df_elbow.hvplot.line(x='k', y='inertia', title='Elbow Curve', xticks=k, width=1000, height=600)
                print(elbow_plot_1)
                
                model = KMeans(n_clusters=4, random_state=0)
                model.fit(df_market_data_scaled)
                clusters = model.predict(df_market_data_scaled)
                print(clusters)
                
                df_market_data_scaled_with_clusters = df_market_data_scaled.copy()
                df_market_data_scaled_with_clusters['cluster'] = clusters
                print(df_market_data_scaled_with_clusters.head())
                
                df_market_data_scaled_with_clusters.hvplot.scatter(x="price_change_percentage_24h", y="price_change_percentage_7d", by="cluster", hover_cols=["coinid"], width=1000, height=600)
                
                pca = PCA(n_components=3)
                crypto_pca = pca.fit_transform(df_market_data_scaled)
                df_crypto_pca = pd.DataFrame(data=crypto_pca, columns=["PC1", "PC2", "PC3"], index=df_market_data_scaled.index)
                print(df_crypto_pca.head())
                explained_variance = pca.explained_variance_ratio_
                print(explained_variance)
                
                df_pca_with_coinid = df_crypto_pca.copy()
                print(df_pca_with_coinid.head())
                
                inertia = []
                for i in k:
                    model = KMeans(n_clusters=i, random_state=0)
                    model.fit(df_pca_with_coinid)
                    inertia.append(model.inertia_)
                
                elbow_data = {"k": k, "inertia": inertia}
                df_elbow2 = pd.DataFrame(elbow_data)
                
                elbow_plot_2 = df_elbow2.hvplot.line(x='k', y='inertia', title='Elbow Curve', xticks=k, width=1000, height=600)
                print(elbow_plot_2)
                
                model = KMeans(n_clusters=4, random_state=0)
                model.fit(df_pca_with_coinid)
                clusters = model.predict(df_pca_with_coinid)
                print(clusters)
                
                df_pca_with_clusters = df_pca_with_coinid.copy()
                df_pca_with_clusters["cluster"] = clusters
                print(df_pca_with_clusters.head())
                
                df_pca_with_clusters.hvplot.scatter(x="PC1", y="PC2", by="cluster", hover_cols=["coinid"], width=1000, height=600)
                
                composite_elbow_plot = elbow_plot_1 * elbow_plot_2
                print(composite_elbow_plot)
                
                composite_cluster_plot = df_market_data_scaled_with_clusters.hvplot.scatter(x="price_change_percentage_24h", y="price_change_percentage_7d", by="cluster", hover_cols=["coinid"], width=1000, height=600) * df_pca_with_clusters.hvplot.scatter(x="PC1", y="PC2", by="cluster", hover_cols=["coinid"], width=1000, height=600)
                print(composite_cluster_plot)
                </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Supervised Learning</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Title: Credit Risk Classification using Logistic Regression<p></p>
                Project Overview:<p></p>
                This project focuses on building a machine learning model to assess credit risk using logistic regression. The dataset comprises historical lending activity from a peer-to-peer lending services company, containing information on borrowers' creditworthiness and loan statuses. The goal is to develop a model that accurately predicts whether a loan is healthy or has a high risk of default, aiding the lending company in making informed decisions regarding loan approvals.
                <p></p>
                Data Source:<p></p>
                The dataset used in this project consists of lending_data.csv, which includes features such as loan amount, interest rate, borrower's credit score, and loan status.
                <p></p>
                Technologies and Methods:<p></p>
                Data Preprocessing: The dataset is preprocessed to create features (X) and labels (y), where a loan status of 0 indicates a healthy loan, and 1 indicates a high-risk loan.<p></p>
                Train-Test Split: The data is split into training and testing sets using the train_test_split function from scikit-learn.<p></p>
                Logistic Regression: A logistic regression model is trained using the training data to classify loans into healthy or high-risk categories.<p></p>
                Model Evaluation: The model's performance is evaluated using metrics such as confusion matrix, accuracy score, precision score, and recall score.<p></p>
                Results:<p></p>
                Accuracy Score: [Insert accuracy score]<p></p>
                Precision Score: [Insert precision score]<p></p>
                Recall Score: [Insert recall score]<p></p>
                Summary:<p></p>
                The logistic regression model demonstrates [Insert performance evaluation]. Based on the results, the model exhibits [Insert strengths or weaknesses]. Considering the importance of accurately assessing credit risk in lending decisions, the model's performance justifies its recommendation for use by the lending company. However, further evaluation and refinement may be necessary to enhance its predictive capabilities and ensure robust risk assessment.
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <p>Plot 1 is a heatmap of the confusion matrix.

                  The horizontal and vertical axes represent the predicted labels and the true labels, where 0 indicates healthy loans and 1 indicates high-risk loans. From the plot, it can be seen that the model performs very well in predicting healthy loans (label 0), with 14,926 samples correctly predicted as healthy loans. However, there is room for improvement in predicting high-risk loans (label 1), with 75 high-risk samples being incorrectly predicted as healthy loans. Overall, there are more instances of misclassifying high-risk loans, indicating that the model needs improvement in this aspect.<p></p>
                <img src="https://i.ibb.co/2YSLKWg/1.png" alt="Excel Showcase">
                <p>Plot 2 is the Precision-Recall Curve.

                  The horizontal axis represents recall, and the vertical axis represents precision. Each point on the curve corresponds to the precision and recall of the model at a particular threshold. An ideal PR curve should approach the top right corner, indicating high precision and recall, suggesting a good balance between the two. From the plot, it can be seen that the model's PR curve performs well, maintaining high precision even at high recall. This indicates that the model can identify high-risk loans well while minimizing the misclassification of healthy loans as high-risk.<p></p>
                <img src="https://i.ibb.co/RP76gMF/2.png" alt="Excel Showcase">
                <p>Plot 3 is the ROC Curve.

                  The horizontal axis represents the false positive rate (FPR), and the vertical axis represents the true positive rate (TPR). The area under the ROC curve (AUC) is a commonly used metric for evaluating the performance of binary classification models, where a higher AUC indicates better overall performance. From the plot, it can be observed that the model's ROC curve is very close to the top left corner, with an AUC of 0.95, indicating excellent overall performance. Across various thresholds, the model can effectively balance the true positive rate and false positive rate.<p></p>
                <img src="https://i.ibb.co/rfSTXZd/3.png" alt="Excel Showcase">
                <p>Overall, these three plots demonstrate that the logistic regression model performs exceptionally well in predicting loan risk, particularly in identifying healthy loans almost perfectly.
                  <p></p>
                
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code># Import the modules
                import numpy as np
                import pandas as pd
                from pathlib import Path
                from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report
                from sklearn.model_selection import train_test_split, GridSearchCV
                from sklearn.linear_model import LogisticRegression
                from imblearn.over_sampling import RandomOverSampler
                
                # Read the lending_data.CSV file from the Resources folder into a Pandas DataFrame
                lending_df = pd.read_csv(Path("Resources/lending_data.csv"))
                
                # Create the labels set (y) from the “loan_status” column, and then create the features (X) DataFrame from the remaining columns
                y = lending_df['loan_status']
                X = lending_df.drop('loan_status', axis=1)
                
                # Check the balance of the labels variable (y)
                target_balance = y.value_counts()
                
                # Split the data into training and testing datasets using train_test_split
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
                
                # Fit a logistic regression model by using the training data (X_train and y_train)
                logistic_regression_model = LogisticRegression(random_state=1)
                param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
                grid_search = GridSearchCV(logistic_regression_model, param_grid, cv=5, scoring='balanced_accuracy')
                grid_search.fit(X_train, y_train)
                best_params = grid_search.best_params_
                logistic_regression_model.set_params(**best_params)
                logistic_regression_model.fit(X_train, y_train)
                
                # Save the predictions on the testing data labels by using the testing feature data (X_test) and the fitted model
                predictions = logistic_regression_model.predict(X_test)
                
                # Evaluate the model’s performance by calculating the accuracy score, generating a confusion matrix, and printing the classification report
                balanced_accuracy = balanced_accuracy_score(y_test, predictions)
                conf_matrix = confusion_matrix(y_test, predictions)
                report = classification_report(y_test, predictions)
                
                # Use the RandomOverSampler to resample the data
                random_oversampler = RandomOverSampler(random_state=1)
                X_train_resampled, y_train_resampled = random_oversampler.fit_resample(X_train, y_train)
                
                # Fit the Logistic Regression model using the resampled training data
                logistic_regression_model.fit(X_train_resampled, y_train_resampled)
                
                # Make predictions using the resampled testing data
                predictions_resampled = logistic_regression_model.predict(X_test)
                
                # Evaluate the model's performance after resampling
                balanced_accuracy_resampled = balanced_accuracy_score(y_test, predictions_resampled)
                conf_matrix_resampled = confusion_matrix(y_test, predictions_resampled)
                report_resampled = classification_report(y_test, predictions_resampled)
                </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Neural Networks, Deep Learning</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Name: Alphabet Soup Foundation Funding Success Predictor<p></p>
                Project Overview<p></p>
                In this project, we developed a predictive tool for the Alphabet Soup Foundation, aimed at optimizing their funding allocation decisions by predicting the success of funded organizations. Leveraging machine learning algorithms to analyze historical funding data, we built a binary classification model that serves to enhance the foundation's social impact through more informed decision-making.
                <p></p>
                Dataset<p></p>
                The dataset utilized comprises records of over 34,000 organizations, each characterized by multiple features such as organization type, industry affiliation, use case for funding, active status, income classification, and requested funding amount. This data represents real cases of organizations that were supported financially by the Alphabet Soup Foundation over several years.
                <p></p>
                Methodology<p></p>
                The project was executed in several key phases:
                <p></p>
                Data Preprocessing<p></p>
                Data was cleaned, including the removal of irrelevant identifier features (such as organizations' EIN and names).<p></p>
                Categorical variables were encoded, and sparse categories were managed.<p></p>
                The data was split into training and testing sets, followed by feature standardization.<p></p>
                Model Development and Training<p></p>
                A deep neural network was constructed using TensorFlow and Keras frameworks.<p></p>
                Optimal layers and neuron counts were determined through experimentation.<p></p>
                The model was compiled with callbacks for progress saving.<p></p>
                The model underwent training and was evaluated on a test set.<p></p>
                Model Optimization<p></p>
                Various strategies were employed to optimize model performance, including adjusting input features, adding hidden layers, modifying activation functions, and altering training epochs.<p></p>
                A series of optimization experiments ensured the model's predictive accuracy surpassed 75%.<p></p>
                Analysis of Results<p></p>
                The model demonstrated high predictive accuracy, supporting the foundation's future funding allocation decisions.<p></p>
                Outcome<p></p>
                Ultimately, we successfully developed a reliable predictive model and packaged it as an end-to-end solution ready for integration into the Alphabet Soup Foundation's decision-making process to improve the effective use of funds.
                <p></p>
                Technology Stack<p></p>
                Data Processing and Analysis: Pandas<p></p>
                Machine Learning Modeling: scikit-learn, TensorFlow, Keras<p></p>
                Data Visualization: Matplotlib, Seaborn<p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code># import dependencies
                from sklearn.model_selection import train_test_split
                from sklearn.preprocessing import StandardScaler
                import pandas as pd
                import tensorflow as tf
                
                #  Import and read the charity_data.csv.
                import pandas as pd 
                application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
                application_df.head()
                
                # Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
                application_df = application_df.drop(columns=["EIN", "NAME"])
                
                # show the first 5 rows of the dataframe
                application_df.head()
                
                # Determine the number of unique values in each column.
                application_df.nunique()
                
                # Look at APPLICATION_TYPE value counts for binning
                application_type_counts = application_df.APPLICATION_TYPE.value_counts()
                application_type_counts
                
                # Choose a cutoff value and create a list of application types to be replaced
                # use the variable name `application_types_to_replace`
                application_types_to_replace = list(application_type_counts[application_type_counts < 500].index)
                
                # Replace in dataframe
                for app in application_types_to_replace:
                    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")
                
                # Check to make sure binning was successful
                application_df['APPLICATION_TYPE'].value_counts()
                
                # Look at CLASSIFICATION value counts for binning
                classification_counts = application_df.CLASSIFICATION.value_counts()
                classification_counts
                
                # Choose a cutoff value and create a list of classifications to be replaced
                # use the variable name `classifications_to_replace`
                classifications_to_replace = list(classification_counts[classification_counts < 1000].index)
                
                # Replace in dataframe
                for cls in classifications_to_replace:
                    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,"Other")
                    
                # Check to make sure binning was successful
                application_df['CLASSIFICATION'].value_counts()
                
                # Convert categorical data to numeric with `pd.get_dummies`
                application_df = pd.get_dummies(application_df)
                
                print(application_df.head()) 
                
                # Split our preprocessed data into our features and target arrays
                y = application_df["IS_SUCCESSFUL"].values
                
                # Split the preprocessed data into a training and testing dataset
                X = application_df.drop(columns=["IS_SUCCESSFUL"]).values
                X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)
                
                # Create a StandardScaler instances
                scaler = StandardScaler()
                
                # Fit the StandardScaler
                X_scaler = scaler.fit(X_train)
                
                # Scale the data
                X_train_scaled = X_scaler.transform(X_train)
                X_test_scaled = X_scaler.transform(X_test)
                
                
                from tensorflow.keras.models import Sequential
                from tensorflow.keras.layers import Dense
                
                model = Sequential()
                model.add(Dense(80, activation='relu', input_dim=X_train_scaled.shape[1]))
                model.add(Dense(40, activation='relu'))
                model.add(Dense(20, activation='relu'))
                model.add(Dense(1, activation='sigmoid'))
                
                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
                
                loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=2)
                print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')
                
                
                </code></pre></div>
          </div>
      </div>
  </div>
</div>

<div class="skill-item">
  <div class="accordion-item">
      <button class="accordion-button">Big Data</button>
      <div class="accordion-content">
          <!-- 子手风琴按钮：项目描述 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Description</button>
              <div class="accordion-content"><p>Project Overview: Analysis and Optimization of Home Sales Data<p></p>
                Background:<p></p>
                The real estate market has always been a topic of great interest, with crucial insights needed by investors and homebuyers alike to understand market trends and key metrics. This project aims to leverage SparkSQL technology to perform in-depth analysis of home sales data, extract valuable insights, and enhance data processing efficiency through optimization techniques.
                <p></p>
                Project Objectives:<p></p>
                Analyze home sales data using SparkSQL to extract key metrics.<p></p>
                Improve data processing speed through caching and data partitioning optimization techniques.<p></p>
                Establish an efficient data processing workflow to enhance data analysis and querying efficiency.<p></p>
                Data Source:<p></p>
                The dataset used in this project is home sales data, which includes various attributes and sales information of homes. The dataset has undergone preprocessing, including cleaning and formatting.
                <p></p>
                Technical Implementation:<p></p>
                Utilize PySpark's SparkSQL module to read and process the data.<p></p>
                Create temporary tables and views using SparkSQL for data analysis and querying.<p></p>
                Improve data query performance using caching techniques.<p></p>
                Optimize data storage and querying efficiency through data partitioning.<p></p>
                Project Workflow:<p></p>
                Data Reading and Preprocessing: Use PySpark to read home sales data and perform necessary preprocessing operations, such as removing irrelevant columns and handling missing values.<p></p>
                Data Analysis: Utilize SparkSQL to perform various data analyses, including calculating average prices, analyzing sales trends, etc.<p></p>
                Data Optimization: Improve data processing speed and querying efficiency through caching techniques and data partitioning optimization.<p></p>
                Results Visualization: Visualize analysis results to provide clear and intuitive insights to users.<p></p>
                Results Presentation:<p></p>
                Average Price Trend Chart: Display the average prices of different types of homes over time to visualize price trends.<p></p>
                Query Performance Comparison Chart: Compare query performance before and after optimization to demonstrate the effectiveness of optimization.<p></p>
                Interactive Data Analysis Interface: Build an interactive data analysis interface where users can customize query conditions and visualization methods to obtain desired insights.<p></p>
                Conclusion and Outlook:<p></p>
                Through the implementation of this project, we can gain deeper insights into the operation of the real estate market, providing investors and buyers with more accurate data references. Additionally, the optimized data processing workflow supports more efficient data analysis tasks. In the future, we will continue to optimize the data processing workflow to enhance system performance and expand data analysis capabilities to meet broader business needs.
                <p></p>
                Technologies Used:<p></p>
                PySpark<p></p>
                SparkSQL<p></p>
                Data caching techniques<p></p>
                Data partitioning optimization techniques<p></p>
                </p></div>
          </div>
          <!-- 子手风琴按钮：项目展示 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Showcase</button>
              <div class="accordion-content">
                <img src="https://i.ibb.co/mcxXkSG/1.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/q53dkqk/2.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/6yZhd0t/3.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/jws16Dp/4.png" alt="Excel Showcase">
                <img src="https://i.ibb.co/pRgWNsQ/5.png" alt="Excel Showcase">
                
              </div>
          </div>
          <!-- 子手风琴按钮：项目代码 -->
          <div class="accordion-item">
              <button class="accordion-button">Project Code</button>
              <div class="accordion-content"><pre><code>import findspark
                findspark.init()
                
                from pyspark.sql import SparkSession
                import time
                
                # Initialize SparkSession
                spark = SparkSession.builder.appName("SparkSQL").getOrCreate()
                
                # Read data from AWS S3 bucket into a DataFrame
                from pyspark import SparkFiles
                url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"
                spark.sparkContext.addFile(url)
                df = spark.read.csv(SparkFiles.get("home_sales_revised.csv"), sep=",", header=True)
                df.show()
                
                # Create a temporary view of the DataFrame
                df.createOrReplaceTempView("home_sales_view")
                result = spark.sql("SELECT * FROM home_sales_view LIMIT 10")
                result.show()
                
                # Query 3: Calculate the average price for a four bedroom house sold per year, rounded to two decimal places
                query = """
                SELECT 
                    YEAR(date) as YearSold, 
                    ROUND(AVG(price), 2) as AvgPrice
                FROM 
                    home_sales_view
                WHERE 
                    bedrooms = '4'
                GROUP BY 
                    YEAR(date)
                ORDER BY 
                    YearSold
                """
                avg_price_result = spark.sql(query)
                avg_price_result.show()
                
                # Query 4: Calculate the average price of a home for each year it was built with 3 bedrooms and 3 bathrooms
                query = """
                SELECT 
                    date_built as YearBuilt, 
                    ROUND(AVG(price), 2) as AvgPrice
                FROM 
                    home_sales_view
                WHERE 
                    bedrooms = '3' AND bathrooms = '3'
                GROUP BY 
                    date_built
                ORDER BY 
                    YearBuilt
                """
                avg_price_per_year_built = spark.sql(query)
                avg_price_per_year_built.show()
                
                # Query 5: Calculate the average price of a home for each year it was built with specific criteria
                query_3bed_3bath_2floors_2000sqft = """
                SELECT 
                    date_built as YearBuilt, 
                    ROUND(AVG(price), 2) as AvgPrice
                FROM 
                    home_sales_view
                WHERE 
                    bedrooms = '3' AND 
                    bathrooms = '3' AND
                    floors = '2' AND
                    sqft_living >= 2000
                GROUP BY 
                    date_built
                ORDER BY 
                    YearBuilt
                """
                avg_price_3bed_3bath_2floors_2000sqft_result = spark.sql(query_3bed_3bath_2floors_2000sqft)
                avg_price_3bed_3bath_2floors_2000sqft_result.show()
                
                # Query 6: Calculate the average price of a home per "view" rating with specific criteria and measure runtime
                start_time = time.time()
                query_task6 = """
                SELECT 
                    view as ViewRating, 
                    ROUND(AVG(price), 2) as AvgPrice
                FROM 
                    home_sales_view
                WHERE 
                    price >= 350000
                GROUP BY 
                    view
                HAVING 
                    AVG(price) >= 350000
                ORDER BY 
                    ViewRating DESC
                """
                avg_price_result_task6 = spark.sql(query_task6)
                avg_price_result_task6.show()
                print("--- %s seconds ---" % (time.time() - start_time))
                
                # Cache the home_sales table
                spark.catalog.cacheTable("home_sales_view")
                
                # Check if the table is cached
                is_cached = spark.catalog.isCached('home_sales_view')
                print(is_cached)
                
                # Query 9: Run the last query on cached data and compare runtime
                start_time = time.time()
                query = """
                SELECT 
                    view as ViewRating, 
                    ROUND(AVG(price), 2) as AvgPrice
                FROM 
                    home_sales_view
                WHERE 
                    price >= 350000
                GROUP BY 
                    view
                HAVING 
                    AVG(price) >= 350000
                ORDER BY 
                    ViewRating DESC
                """
                avg_price_result = spark.sql(query)
                avg_price_result.show()
                runtime = time.time() - start_time
                print("--- %s seconds ---" % runtime)
                
                # Write partitioned parquet data
                df.write.partitionBy("date_built").parquet("partitioned_home_sales.parquet", mode="overwrite")
                
                # Read partitioned parquet data
                df_parquet = spark.read.parquet("partitioned_home_sales.parquet")
                df_parquet.show()
                
                # Create temporary table for parquet data
                df_parquet.createOrReplaceTempView("parquet_home_sales_view")
                result = spark.sql("SELECT * FROM parquet_home_sales_view LIMIT 10")
                result.show()
                
                # Query 13: Run the last query on parquet data and compare runtime
                start_time = time.time()
                query = """
                SELECT 
                    view as ViewRating, 
                    ROUND(AVG(price), 2) as AvgPrice
                FROM 
                    parquet_home_sales_view
                WHERE 
                    price >= 350000
                GROUP BY 
                    view
                HAVING 
                    AVG(price) >= 350000
                ORDER BY 
                    ViewRating DESC
                """
                avg_price_result = spark.sql(query)
                avg_price_result.show()
                runtime = time.time() - start_time
                print(f"--- {runtime} seconds ---")
                
                # Uncache the home_sales table
                spark.catalog.uncacheTable("home_sales_view")
                
                # Check if the table is no longer cached
                is_cached = spark.catalog.isCached('home_sales_view')
                print(is_cached)
                </code></pre></div>
          </div>
      </div>
  </div>
</div>

  <!-- 在这里添加更多的技能项 -->

  <section class="skills">
    <div class="project-title-container">
        <h2>My Projects 1</h2>
    </div>
</section>

  <section class="projects">
    <div class="project-card">
      <div class="project-card-bg" style="background-image: url('https://www.expeditionsalaska.com/wp-content/uploads/2016/09/13\_Oct-polar-bears-5626.jpg');"></div>
      <div class="project-info">
        <h3>Alaskan Polar Bear Data Analysis</h3>
        <p>
          Introduction
          In the face of rapidly changing global climates and increasing human encroachments into natural habitats, understanding the impact on wildlife, especially on vulnerable species like polar bears, is crucial. Our project, "Alaskan Polar Bear Data Analysis," leverages modern web technologies and data visualization tools to present a comprehensive analysis of the factors affecting polar bear populations in Alaska. This multidisciplinary approach combines environmental science, data analysis, and web development, offering insights into how climate change, human activities, and environmental pollution influence polar bear habitats and behavior.
          <p></p>
          Technologies Employed
          Leaflet.js: Utilized for interactive mapping, Leaflet.js allows us to visually represent the geographic distribution of maternal dens and movement tracks of polar bears across Alaska. This interactive component enhances user engagement, providing a spatial understanding of polar bear habitats.
          
          D3.js: A JavaScript library for producing dynamic, interactive data visualizations in web browsers. We employ D3.js to create sophisticated graphical representations, such as pie charts and line graphs, to depict the correlation between polar bear population dynamics and various environmental factors.
          
          Plotly.js: Integrated for its advanced plotting capabilities, Plotly.js aids in the visualization of complex datasets, enabling us to present detailed analyses of climate change effects on polar bear populations through interactive charts.
          
          Chart.js: This lightweight library is used to render graphical data in a clear and appealing manner, supporting the project's goal to communicate scientific findings effectively to a broad audience.
          <p></p>
          Content Highlights<p></p>
          Population Analysis: An in-depth examination of Alaska's polar bear population trends over the years, correlating these trends with factors such as human population growth, industrial activities, and environmental policies.
          
          Impact of Chemical Pollution: A critical analysis of how pollutants affect polar bear health and habitat, supported by data on contaminant levels in captured bears and the resultant health implications.
          
          Climate Change Effects: Utilizing temperature and ice coverage data, this section outlines the direct and indirect effects of climate change on polar bear habitats, migration patterns, and access to food sources.
          
          Interactive Visualizations: Through the use of Leaflet.js, D3.js, Plotly.js, and Chart.js, the project presents an interactive web-based platform that allows users to explore data on polar bear populations, pollution levels, and climate change impacts visually.
          <p></p>
          Conclusion<p></p>
          The "Alaskan Polar Bear Data Analysis" project stands as a testament to the power of integrating data science with web development to address critical environmental issues. By providing a visually engaging and informative analysis, the project aims to raise awareness and stimulate discussion on the preservation of polar bear populations and their habitats amidst the challenges posed by climate change and human activities. This work serves as a pivotal resource for educators, researchers, policymakers, and the general public, fostering a deeper understanding of the intricate balance between nature and human society.<p></p>
          
          <a href="https://sitexiang.github.io/pp2/" target="_blank">Click to navigate to the "Alaskan Polar Bear Project" page</a>
      </div>
    </div>
</section>

<section class="skills">
  <div class="center-content">
      <h2>My Projects 2</h2>
  </div>
</section>

<section class="projects">
  <div class="project-card project2">
    <div class="project-card-bg" style="background-image: url('https://i.ibb.co/CzgmC8b/2.png');"></div>
    <div class="project-info">
      <h3>iStar Virtual Smartphone Initiative: Bridging Strategy with Technology</h3>
      <p>Executive Summary:<p></p>
        The iStar Virtual Smartphone Project marks a significant leap in smartphone innovation, blending strategic market insights with sophisticated technological integrations to launch the novel iStar smartphone brand. With a phased approach that encompasses extensive market analysis, consumer sentiment analytics, predictive modeling, and post-launch feedback processing, the project showcases an exemplary fusion of market acumen and technological prowess.
        <p></p>
        Strategic Framework and Technological Mastery:</p>
        Market Insight and Entry Strategy: Initiating with an exhaustive market analysis for 2023 and 2024, the project employs Pandas and Numpy for data manipulation and analysis, underpinned by Flask for deploying interactive visualizations. This phase deciphers market share distributions and trend trajectories, providing iStar with a data-driven entry strategy.
        <p></p>
        Consumer Sentiment Analysis: </p>
        Leveraging Natural Language Processing (NLP) technologies, including NLTK for text processing and Gensim’s Word2Vec for text embedding, alongside PyTorch’s neural networks, the project dives deep into customer reviews. This dual-approach not only uncovers customer satisfaction levels but also extracts key themes, guiding feature development based on user preferences.
        <p></p>
        Innovative Price Prediction Modeling: </p>
        At the heart of the iStar initiative is a dynamic price prediction model developed using Scikit-learn and StandardScaler for predictive analytics and data preprocessing. This model, informed by specifications like chipset, display, and camera quality, empowers iStar with competitive pricing strategies.
        <p></p>
        Post-Launch Sentiment and Theme Dual-Model: </p>
        Anticipating the need for ongoing product refinement, a sophisticated dual-model system combines sentiment analysis with theme extraction, processed through PyTorch and advanced NLP techniques. This model ensures a comprehensive understanding of consumer feedback for continuous product improvement.
        <p></p>
        Deployment and Integration:</p>
        The culmination of the iStar project is its integration into the iStar brand’s digital ecosystem, featuring a suite of interactive tools powered by Flask and CORS. This not only facilitates an engaging user experience but also solidifies iStar’s positioning as a tech-forward, user-centric smartphone brand.
        <p></p>
        Conclusion:</p>
        The iStar Virtual Smartphone Project exemplifies the synergy between strategic market planning and technological innovation. By harnessing advanced analytics, machine learning, and NLP, iStar is poised to not just enter but redefine the smartphone market, offering products that resonate deeply with consumer needs and setting new benchmarks for quality and innovation.<p></p>
        
        <a href="https://sitexiang.github.io/iStarcellphones/" target="_blank">Click to navigate to the "iStar Virtual Smartphone" page</a>
    </div>
  </div>
</section>

<section class="skills">
    <div class="center-content">
        <h2>My Projects 3</h2>
    </div>
  </section>

<section class="projects">
  <div class="project-card project3">
    <div class="project-card-bg" style="background-image: url('https://i.ibb.co/zXNdc9H/bg1.jpg');"></div>
    <div class="project-info">
      <h3>An analysis of Titanic survivors</h3>
      <p>Introduction:<p></p>
        This project delves into a comprehensive analysis and visualization of Titanic survivor data to investigate various factors influencing survival rates. Through a combination of data visualization, statistical analysis, and machine learning model evaluation, this project reveals the complex patterns and societal dynamics behind the survivors of the Titanic disaster. Below are the key components and highlights of this project:
        <p></p>
        Data Analysis and Visualization:<p></p>
        
        Analyzing factors such as gender, passenger class, age, etc., and their impact on survival rates. Visualizations vividly demonstrate the correlations and degrees of influence among different factors.
        <p></p>
        Machine Learning Model Evaluation:<p></p>
        
        Employing machine learning models to predict survival probabilities and assessing the performance of different models. Through methods like cross-validation and feature importance analysis, exploring which factors have significant impacts on prediction outcomes.
        <p></p>
        Multidimensional Analysis:<p></p>
        
        Beyond singular factor analysis, exploring interactions among different factors, such as the intersection of gender and age, the combined impact of family size and embarkation port on survival rates, etc.
        <p></p>
        Conclusions and Insights:<p></p>
        
        Drawing deep insights and reflections on the societal structures and biases underlying survival rates during the Titanic disaster. This project goes beyond a mere retrospective of a historical event, offering profound reflections on society and human nature.<p></p>
        This project showcases the application of data science in exploring historical events and societal phenomena, providing profound insights and perspectives. Through this project, we gain a better understanding of the complex motivations behind human behavior and offer valuable insights and lessons for the prevention and rescue efforts of similar events in the future.<p></p>
        
        <a href="https://sitexiang.github.io/titanicwebs/" target="_blank">Click to navigate to the "Titanic Survivors Analysis" page</a>
    </div>
  </div>
</section>
<script src="js/main.js"></script>
</body>
</html>